{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Welcome to the QCon 2022 Istio service mesh workshop! On this site you will find the hands-on labs for the workshop. In the first lab, we walk you through accessing and configuring your lab environment . Let's begin.","title":"Welcome!"},{"location":"#welcome","text":"Welcome to the QCon 2022 Istio service mesh workshop! On this site you will find the hands-on labs for the workshop. In the first lab, we walk you through accessing and configuring your lab environment . Let's begin.","title":"Welcome!"},{"location":"circuit_breakers/","text":"Circuit breakers \u00b6 This lab demonstrate how to configure circuit breaking both with and without outlier detection in Istio. Prerequisites and setup \u00b6 Kubernetes with Istio and other tools (Prometheus, Zipkin, Grafana) installed web-frontend and customers workloads already deployed and running. Collect additional metrics \u00b6 Modify the installation of Istio to enable collection of additional metrics: istio-metrics.yaml 1 2 3 4 5 6 7 8 9 apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : profile : demo meshConfig : defaultConfig : proxyStatsMatcher : inclusionPrefixes : - \"cluster.outbound\" The above will enable collection of metrics for all prefixes in the list and for all workloads in the mesh. Info This is not recommended for production environments as the number of metrics collected will be very large. Typically, we'd constrain the metrics collection of extra metrics to a specific workload. For example, we could have enabled those metrics on the Fortio deployment we'll use to generate the load, so the metrics would only be collected for that workload). Save the above to istio-metrics.yaml and apply the configuration to your Istio installation: istioctl install -f istio-metrics.yaml Install Fortio \u00b6 Let us try and generate some load to the web-frontend workload and see the distribution of responses. We'll use Fortio to generate load on the web-frontend service. Deploy Fortio Click for fortio.yaml fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion : v1 kind : Service metadata : name : fortio labels : app : fortio service : fortio spec : ports : - port : 8080 name : http selector : app : fortio --- apiVersion : apps/v1 kind : Deployment metadata : name : fortio-deploy spec : replicas : 1 selector : matchLabels : app : fortio template : metadata : labels : app : fortio spec : containers : - name : fortio image : fortio/fortio:latest_release imagePullPolicy : Always ports : - containerPort : 8080 name : http-fortio - containerPort : 8079 name : grpc-ping Save the above file to fortio.yaml and deploy it: kubectl apply -f fortio.yaml Make a single request to make sure everything is working: export FORTIO_POD = $( kubectl get pods -l app = fortio -o 'jsonpath={.items[0].metadata.name}' ) Then: kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio curl http://web-frontend The output should resemble this: ... HTTP/1.1 200 OK x-powered-by: Express content-type: text/html; charset=utf-8 content-length: 2471 etag: W/\"9a7-hEXE7lJW5CDgD+e2FypGgChcgho\" x-envoy-upstream-service-time: 28 server: envoy Circuit breaker - connection pool settings \u00b6 Study the following DestionationRule: cb-web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 1 # (1) http2MaxRequests : 1 # (2) maxRequestsPerConnection : 1 # (3) The maximum number of pending HTTP requests to a destination. The maximum number of concurrent requests to a destination. The maximum number of requests per connection. It configures the connection pool for web-frontend with very low thresholds, to easily trigger the circuit breaker. Save the above YAML to cb-web-frontend.yaml and apply the changes: kubectl apply -f cb-web-frontend.yaml Since all values are set to 1, we won't trigger the circuit breaker if we send the request using one connection and one request per second. If we increase the number of connections and send more requests (i.e. 2 workers sending requests concurrently, and sending 50 requests), we'll start getting errors. The errors happen because the http2MaxRequests is set to 1 and we have more than 1 concurrent request being sent. Additonally, we're exceeding the maxRequestsPerConnection limit. kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 24 (48.0 %) Code 503 : 26 (52.0 %) Tip To reset the metric counters, run kubectl exec $FORTIO_POD -c istio-proxy -- curl -X POST localhost:15000/reset_counters Observe failures in Zipkin \u00b6 Open the Zipkin dashboard: istioctl dash zipkin In the Zipkin UI, click the Run Query button and pick a failing trace to see the details. You can identify failing traces by looking at the number of spans - the failing trace will have 1 span, while the successful ones will have 4 spans. The requests are failing because the circuit breaker is tripped. Response flags are set to UO (Upstream Overflow) and the status code is 503 (service unavailable). Prometheus metrics \u00b6 Another option is looking at the Prometheus metrics directly. Open the Prometheus dashboard: istioctl dash prometheus Apply the following PromQL query: envoy_cluster_upstream_rq_pending_overflow { app = \" fortio \", cluster_name = \" outbound|80||web-frontend.default.svc.cluster.local \"} The upstream_rq_pending_overflow and other metrics are described here . The query shows the metrics for requests originating from the fortio app and going to the web-frontend service. We can also look at the metrics directly from the istio-proxy container in the Fortio Pod: kubectl exec \" $FORTIO_POD \" -c istio-proxy -- pilot-agent request GET stats | grep web-frontend | grep pending cluster.outbound|80||web-frontend.default.svc.cluster.local.circuit_breakers.default.remaining_pending: 1 cluster.outbound|80||web-frontend.default.svc.cluster.local.circuit_breakers.default.rq_pending_open: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.circuit_breakers.high.rq_pending_open: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_active: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_failure_eject: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_overflow: 26 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_total: 24 To resolve these errors, we can adjust the circuit breaker settings. Increase the maximum number of concurrent requests to 2 ( http2MaxRequests ), as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 1 http2MaxRequests : 2 maxRequestsPerConnection : 1 Save the above YAML to cb-web-frontend.yaml and apply the changes: kubectl apply -f cb-web-frontend.yaml If we re-run Fortio with the same parameters, we'll notice less failures this time: kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 39 (78.0 %) Code 503 : 11 (22.0 %) Since we're sending more than 1 request per connection, we can increase the maxRequestsPerConnection to 2: 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 1 http2MaxRequests : 2 maxRequestsPerConnection : 2 Save the above YAML to cb-web-frontend.yaml and apply the changes: kubectl apply -f cb-web-frontend.yaml If we re-run Fortio this time, we'll get zero or close to zero HTTP 503 reponses. Even if we increase the number of requests per second, we should only get a small number of 503 responses. To get rid of the remaining failing requests, we can increase the http1MaxPendingRequests to 2: 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 2 http2MaxRequests : 2 maxRequestsPerConnection : 2 With these settings (assuming 2 concurrent connections), we can easily handle a higher number of requests. To be clear, the numbers we used in settings are just examples and are not realistic - we set them intentionally low to make the circuit breaker easier to trip. Before continuing, delete the DestinationRule: kubectl delete destinationrule web-frontend Reset the metric counters: kubectl exec $FORTIO_POD -c istio-proxy -- curl -X POST localhost:15000/reset_counters Outlier detection \u00b6 The circuit breaker is great when we want to protect the services from a sudden burst of requests. However, how can we protect the services in case of failures? For example, if we have a service that is still failing after multiple requests, it doesn't make sense to send even more requests to it. Instead, we can remove the instance of the failing service from the load balancing pool for a certain period of time. That way, we know that the requests will go to other instances of the service. After a pre-defined period of time, we can bring the failing service back into the load balancing pool. This process is called outlier detection. Just like in the connection pool settings, we can configure outlier detection in the DestinationRule. To see the outlier detection in action we need a service that is failing. We'll create a web-frontend-failing deployment and configure it to return HTTP 503 responses: Click for web-frontend-failing.yaml web-frontend-failing.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend-failing labels : app : web-frontend spec : replicas : 4 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : 'http://customers.default.svc.cluster.local' - name : ERROR_RATE value : '100' - name : ERROR_STATUS_CODE value : '500' Save the above YAML to web-frontend-failing.yaml and apply it to the cluster: kubectl apply -f web-frontend-failing.yaml If we run Fortio we'll see that majority of the requests will be failing. That's because the web-frontend-failing deployment has more replicas than the \"good\" deployment. kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 9 (18.0 %) Code 500 : 41 (82.0 %) Let's look at an example of outlier detection configuration: outlier-web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : outlierDetection : consecutive5xxErrors : 1 # (1) interval : 5s # (2) baseEjectionTime : 60s # (3) maxEjectionPercent : 100 # (4) Number of 5xx errors in a row that will trigger the outlier detection. The interval at which the hosts are checked whether they need to be ejected. The duration of time an outlier is ejected from the load balancing pool. If the same host is ejected multiple times, the ejection time increases by multiplying the base ejection time by the number of times the host is ejected. The maximum percentage of hosts that can be ejected. Save the YAML to outlier-web-frontend.yaml and apply it: kubectl apply -f outlier-web-frontend.yaml If we repeat the test, we might get a similar distribution of responses the first time, however, if we repeat the command again (once the outliers were kicked out), we'll get a much better distribution: kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 50 (100.0 %) The reason for more HTTP 200 responses is because as soon as the failing hosts were ejected (failing Pods from the web-frontend-failing deployment), the requests were sent to the other host that doesn't fail. If we'd wait for a while (the 60s baseEjectionTime ), the failing hosts would be brought back into the load balancing pool and we'd get a similar distribution of responses as before (majority of them failing). We can also look at the metrics from the outlier detection in the same way we did for the circuit breakers: kubectl exec \" $FORTIO_POD \" -c istio-proxy -- pilot-agent request GET stats | grep web-frontend | grep ejections_total Produces output similar to this: cluster.outbound|80||web-frontend.default.svc.cluster.local.outlier_detection.ejections_total: 4 Note Other metrics that we can look at are ejections_consecutive_5xx , ejections_enforced_total or any other metric with outlier_detection in its name. You can find the full list of metric names and there descriptions in the Envoy documentation . Cleanup \u00b6 To clean up all resources, run: kubectl delete -f web-frontend-failing.yaml kubectl delete -f cb-lab.yaml kubectl delete -f fortio.yaml kubectl delete dr --all kubectl delete vs --all kubectl delete gateway --all","title":"Circuit breakers"},{"location":"circuit_breakers/#circuit-breakers","text":"This lab demonstrate how to configure circuit breaking both with and without outlier detection in Istio.","title":"Circuit breakers"},{"location":"circuit_breakers/#prerequisites-and-setup","text":"Kubernetes with Istio and other tools (Prometheus, Zipkin, Grafana) installed web-frontend and customers workloads already deployed and running.","title":"Prerequisites and setup"},{"location":"circuit_breakers/#collect-additional-metrics","text":"Modify the installation of Istio to enable collection of additional metrics: istio-metrics.yaml 1 2 3 4 5 6 7 8 9 apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : profile : demo meshConfig : defaultConfig : proxyStatsMatcher : inclusionPrefixes : - \"cluster.outbound\" The above will enable collection of metrics for all prefixes in the list and for all workloads in the mesh. Info This is not recommended for production environments as the number of metrics collected will be very large. Typically, we'd constrain the metrics collection of extra metrics to a specific workload. For example, we could have enabled those metrics on the Fortio deployment we'll use to generate the load, so the metrics would only be collected for that workload). Save the above to istio-metrics.yaml and apply the configuration to your Istio installation: istioctl install -f istio-metrics.yaml","title":"Collect additional metrics"},{"location":"circuit_breakers/#install-fortio","text":"Let us try and generate some load to the web-frontend workload and see the distribution of responses. We'll use Fortio to generate load on the web-frontend service. Deploy Fortio Click for fortio.yaml fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion : v1 kind : Service metadata : name : fortio labels : app : fortio service : fortio spec : ports : - port : 8080 name : http selector : app : fortio --- apiVersion : apps/v1 kind : Deployment metadata : name : fortio-deploy spec : replicas : 1 selector : matchLabels : app : fortio template : metadata : labels : app : fortio spec : containers : - name : fortio image : fortio/fortio:latest_release imagePullPolicy : Always ports : - containerPort : 8080 name : http-fortio - containerPort : 8079 name : grpc-ping Save the above file to fortio.yaml and deploy it: kubectl apply -f fortio.yaml Make a single request to make sure everything is working: export FORTIO_POD = $( kubectl get pods -l app = fortio -o 'jsonpath={.items[0].metadata.name}' ) Then: kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio curl http://web-frontend The output should resemble this: ... HTTP/1.1 200 OK x-powered-by: Express content-type: text/html; charset=utf-8 content-length: 2471 etag: W/\"9a7-hEXE7lJW5CDgD+e2FypGgChcgho\" x-envoy-upstream-service-time: 28 server: envoy","title":"Install Fortio"},{"location":"circuit_breakers/#circuit-breaker-connection-pool-settings","text":"Study the following DestionationRule: cb-web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 1 # (1) http2MaxRequests : 1 # (2) maxRequestsPerConnection : 1 # (3) The maximum number of pending HTTP requests to a destination. The maximum number of concurrent requests to a destination. The maximum number of requests per connection. It configures the connection pool for web-frontend with very low thresholds, to easily trigger the circuit breaker. Save the above YAML to cb-web-frontend.yaml and apply the changes: kubectl apply -f cb-web-frontend.yaml Since all values are set to 1, we won't trigger the circuit breaker if we send the request using one connection and one request per second. If we increase the number of connections and send more requests (i.e. 2 workers sending requests concurrently, and sending 50 requests), we'll start getting errors. The errors happen because the http2MaxRequests is set to 1 and we have more than 1 concurrent request being sent. Additonally, we're exceeding the maxRequestsPerConnection limit. kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 24 (48.0 %) Code 503 : 26 (52.0 %) Tip To reset the metric counters, run kubectl exec $FORTIO_POD -c istio-proxy -- curl -X POST localhost:15000/reset_counters","title":"Circuit breaker - connection pool settings"},{"location":"circuit_breakers/#observe-failures-in-zipkin","text":"Open the Zipkin dashboard: istioctl dash zipkin In the Zipkin UI, click the Run Query button and pick a failing trace to see the details. You can identify failing traces by looking at the number of spans - the failing trace will have 1 span, while the successful ones will have 4 spans. The requests are failing because the circuit breaker is tripped. Response flags are set to UO (Upstream Overflow) and the status code is 503 (service unavailable).","title":"Observe failures in Zipkin"},{"location":"circuit_breakers/#prometheus-metrics","text":"Another option is looking at the Prometheus metrics directly. Open the Prometheus dashboard: istioctl dash prometheus Apply the following PromQL query: envoy_cluster_upstream_rq_pending_overflow { app = \" fortio \", cluster_name = \" outbound|80||web-frontend.default.svc.cluster.local \"} The upstream_rq_pending_overflow and other metrics are described here . The query shows the metrics for requests originating from the fortio app and going to the web-frontend service. We can also look at the metrics directly from the istio-proxy container in the Fortio Pod: kubectl exec \" $FORTIO_POD \" -c istio-proxy -- pilot-agent request GET stats | grep web-frontend | grep pending cluster.outbound|80||web-frontend.default.svc.cluster.local.circuit_breakers.default.remaining_pending: 1 cluster.outbound|80||web-frontend.default.svc.cluster.local.circuit_breakers.default.rq_pending_open: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.circuit_breakers.high.rq_pending_open: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_active: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_failure_eject: 0 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_overflow: 26 cluster.outbound|80||web-frontend.default.svc.cluster.local.upstream_rq_pending_total: 24 To resolve these errors, we can adjust the circuit breaker settings. Increase the maximum number of concurrent requests to 2 ( http2MaxRequests ), as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 1 http2MaxRequests : 2 maxRequestsPerConnection : 1 Save the above YAML to cb-web-frontend.yaml and apply the changes: kubectl apply -f cb-web-frontend.yaml If we re-run Fortio with the same parameters, we'll notice less failures this time: kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 39 (78.0 %) Code 503 : 11 (22.0 %) Since we're sending more than 1 request per connection, we can increase the maxRequestsPerConnection to 2: 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 1 http2MaxRequests : 2 maxRequestsPerConnection : 2 Save the above YAML to cb-web-frontend.yaml and apply the changes: kubectl apply -f cb-web-frontend.yaml If we re-run Fortio this time, we'll get zero or close to zero HTTP 503 reponses. Even if we increase the number of requests per second, we should only get a small number of 503 responses. To get rid of the remaining failing requests, we can increase the http1MaxPendingRequests to 2: 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : connectionPool : http : http1MaxPendingRequests : 2 http2MaxRequests : 2 maxRequestsPerConnection : 2 With these settings (assuming 2 concurrent connections), we can easily handle a higher number of requests. To be clear, the numbers we used in settings are just examples and are not realistic - we set them intentionally low to make the circuit breaker easier to trip. Before continuing, delete the DestinationRule: kubectl delete destinationrule web-frontend Reset the metric counters: kubectl exec $FORTIO_POD -c istio-proxy -- curl -X POST localhost:15000/reset_counters","title":"Prometheus metrics"},{"location":"circuit_breakers/#outlier-detection","text":"The circuit breaker is great when we want to protect the services from a sudden burst of requests. However, how can we protect the services in case of failures? For example, if we have a service that is still failing after multiple requests, it doesn't make sense to send even more requests to it. Instead, we can remove the instance of the failing service from the load balancing pool for a certain period of time. That way, we know that the requests will go to other instances of the service. After a pre-defined period of time, we can bring the failing service back into the load balancing pool. This process is called outlier detection. Just like in the connection pool settings, we can configure outlier detection in the DestinationRule. To see the outlier detection in action we need a service that is failing. We'll create a web-frontend-failing deployment and configure it to return HTTP 503 responses: Click for web-frontend-failing.yaml web-frontend-failing.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend-failing labels : app : web-frontend spec : replicas : 4 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : 'http://customers.default.svc.cluster.local' - name : ERROR_RATE value : '100' - name : ERROR_STATUS_CODE value : '500' Save the above YAML to web-frontend-failing.yaml and apply it to the cluster: kubectl apply -f web-frontend-failing.yaml If we run Fortio we'll see that majority of the requests will be failing. That's because the web-frontend-failing deployment has more replicas than the \"good\" deployment. kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 9 (18.0 %) Code 500 : 41 (82.0 %) Let's look at an example of outlier detection configuration: outlier-web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : web-frontend spec : host : web-frontend.default.svc.cluster.local trafficPolicy : outlierDetection : consecutive5xxErrors : 1 # (1) interval : 5s # (2) baseEjectionTime : 60s # (3) maxEjectionPercent : 100 # (4) Number of 5xx errors in a row that will trigger the outlier detection. The interval at which the hosts are checked whether they need to be ejected. The duration of time an outlier is ejected from the load balancing pool. If the same host is ejected multiple times, the ejection time increases by multiplying the base ejection time by the number of times the host is ejected. The maximum percentage of hosts that can be ejected. Save the YAML to outlier-web-frontend.yaml and apply it: kubectl apply -f outlier-web-frontend.yaml If we repeat the test, we might get a similar distribution of responses the first time, however, if we repeat the command again (once the outliers were kicked out), we'll get a much better distribution: kubectl exec \" $FORTIO_POD \" -c fortio -- /usr/bin/fortio load -c 2 -qps 0 -n 50 -loglevel Warning http://web-frontend ... Code 200 : 50 (100.0 %) The reason for more HTTP 200 responses is because as soon as the failing hosts were ejected (failing Pods from the web-frontend-failing deployment), the requests were sent to the other host that doesn't fail. If we'd wait for a while (the 60s baseEjectionTime ), the failing hosts would be brought back into the load balancing pool and we'd get a similar distribution of responses as before (majority of them failing). We can also look at the metrics from the outlier detection in the same way we did for the circuit breakers: kubectl exec \" $FORTIO_POD \" -c istio-proxy -- pilot-agent request GET stats | grep web-frontend | grep ejections_total Produces output similar to this: cluster.outbound|80||web-frontend.default.svc.cluster.local.outlier_detection.ejections_total: 4 Note Other metrics that we can look at are ejections_consecutive_5xx , ejections_enforced_total or any other metric with outlier_detection in its name. You can find the full list of metric names and there descriptions in the Envoy documentation .","title":"Outlier detection"},{"location":"circuit_breakers/#cleanup","text":"To clean up all resources, run: kubectl delete -f web-frontend-failing.yaml kubectl delete -f cb-lab.yaml kubectl delete -f fortio.yaml kubectl delete dr --all kubectl delete vs --all kubectl delete gateway --all","title":"Cleanup"},{"location":"dashboards/","text":"Observability \u00b6 This lab explores one of the main strengths of Istio: observability. The services in our mesh are automatically observable, without adding any burden on developers. Deploy the Addons \u00b6 The Istio distribution provides addons for a number of systems that together provide observability for the service mesh: Zipkin or Jaeger for distributed tracing Prometheus for metrics collection Grafana provides dashboards for monitoring, using Prometheus as the data source Kiali allows us to visualize the mesh These addons are located in the samples/addons/ folder of the distribution. Navigate to the addons directory cd ~/istio-1.18.2/samples/addons Deploy each addon: kubectl apply -f extras/zipkin.yaml kubectl apply -f prometheus.yaml kubectl apply -f grafana.yaml kubectl apply -f kiali.yaml Verify that the istio-system namespace is now running additional workloads for each of the addons. kubectl get pod -n istio-system The istioctl CLI provides convenience commands for accessing the web UIs for each dashboard. Take a moment to review the help information for the istioctl dashboard command: istioctl dashboard --help Generate a load \u00b6 In order to have something to observe, we need to generate a load on our system. Install a load generator \u00b6 Install a simple load generating tool named siege . We normally install siege with the apt-get package manager. However, given the cloud shell's ephemeral nature, anything installed outside our home directory will vanish after a session timeout. Alternatives: Install from source. It's a little more work, but does not exhibit the above-mentioned problem. Run the load generator from your laptop. On a mac, using homebrew the command is brew install siege . Use a simple bash curl command wrapped in a while loop: while true ; do curl $GATEWAY_IP | head ; sleep 1s ; done Here are the steps to install from source: Fetch the package wget http://download.joedog.org/siege/siege-latest.tar.gz Unpack it tar -xzf siege-latest.tar.gz Navigate into the siege subdirectory with cd siege Tab Run the configure script, and request that siege get installed inside your home directory ./configure --prefix = $HOME Build the code make Finally, install (copies the binary to ~/bin ) make install Feel free to delete (or preserve) the downloaded tar file and source code. Generate a load \u00b6 With siege now installed, familiarize yourself with the command and its options. siege --help Run the following command to generate a mild load against the application. siege --delay = 3 --concurrent = 3 http:// $GATEWAY_IP / Note The siege command stays in the foreground while it runs. It may be simplest to leave it running, and open a separate terminal in your cloud shell environment. Kiali \u00b6 Launch the Kiali dashboard: istioctl dashboard kiali Warning If the dashboard page fails to open, just click on the hyperlink in the console output. Note The istioctl dashboard command also blocks. Leave it running until you're finished using the dashboard, at which time pressing Ctrl + C can interrupt the process and put you back at the terminal prompt. The Kiali dashboard displays. Customize the view as follows: Select the Graph section from the sidebar. Under Select Namespaces (at the top of the page), select the default namespace, the location where the application's pods are running. From the third \"pulldown\" menu, select App graph . From the Display \"pulldown\", toggle on Traffic Animation and Security . From the footer, toggle the legend so that it is visible. Take a moment to familiarize yourself with the legend. Observe the visualization and note the following: We can see traffic coming in through the ingress gateway to the web-frontend , and the subsequent calls from the web-frontend to the customers service The lines connecting the services are green, indicating healthy requests The small lock icon on each edge in the graph indicates that the traffic is secured with mutual TLS Such visualizations are helpful with understanding the flow of requests in the mesh, and with diagnosis. Feel free to spend more time exploring Kiali. We will revisit Kiali in a later lab to visualize traffic shifting such as when performing a blue-green or canary deployment. Kiali Cleanup \u00b6 Close the Kiali dashboard. Interrupt the istioctl dashboard kiali command by pressing Ctrl + C . Zipkin \u00b6 Launch the Zipkin dashboard: istioctl dashboard zipkin The Zipkin dashboard displays. Click on the red '+' button and select serviceName . Select the service named web-frontend.default and click on the Run Query button (lightblue) to the right. A number of query results will display. Each row is expandable and will display more detail in terms of the services participating in that particular trace. Click the Show button to the right of one of the traces having four (4) spans. The resulting view shows spans that are part of the trace, and more importantly how much time was spent within each span. Such information can help diagnose slow requests and pin-point where the latency lies. Distributed tracing also helps us make sense of the flow of requests in a microservice architecture. Zipkin Cleanup \u00b6 Close the Zipkin dashboard. Interrupt the istioctl dashboard zipkin command with Ctrl + C . Prometheus \u00b6 Prometheus works by periodically calling a metrics endpoint against each running service (this endpoint is termed the \"scrape\" endpoint). Developers normally have to instrument their applications to expose such an endpoint and return metrics information in the format the Prometheus expects. With Istio, this is done automatically by the Envoy sidecar. Observe how Envoy exposes a Prometheus scrape endpoint \u00b6 Run the following command: kubectl exec svc/customers -- curl -s localhost:15020/stats/prometheus | grep istio_requests Why port 15020? See Ports used by Istio sidecar proxy. The list of metrics returned by the endpoint is rather lengthy, so we just peek at \"istio_requests\" metric. The full response contains many more metrics. Access the dashboard \u00b6 Start the prometheus dashboard istioctl dashboard prometheus In the search field enter the metric named istio_requests_total , and click the Execute button (on the right). Select the tab named Graph to obtain a graphical representation of this metric over time. Note that you are looking at requests across the entire mesh, i.e. this includes both requests to web-frontend and to customers . As an example of Prometheus' dimensional metrics capability, we can ask for total requests having a response code of 200: istio_requests_total{response_code=\"200\"} With respect to requests, it's more interesting to look at the rate of incoming requests over a time window. Try: rate(istio_requests_total[5m]) There's much more to the Prometheus query language ( this may be a good place to start). Grafana consumes these metrics to produce graphs on our behalf. Close the Prometheus dashboard and terminate the corresponding istioctl dashboard command. Grafana \u00b6 Launch the Grafana dashboard istioctl dashboard grafana From the sidebar, select Dashboards \u2192 Browse Click on the folder named Istio to reveal pre-designed Istio-specific Grafana dashboards Explore the Istio Mesh Dashboard. Note the Global Request Volume and Global Success Rate. Explore the Istio Service Dashboard. First select the service web-frontend and inspect its metrics, then switch to the customers service and review its dashboard. Explore the Istio Workload Dashboard. Select the web-frontend workload. Look at Outbound Services and note the outbound requests to the customers service. Select the customers workload and note that it makes no Oubtound Services calls. Feel free to further explore these dashboards. Cleanup \u00b6 Terminate the istioctl dashboard command ( Ctrl + C ) Likewise, terminate the siege command Next \u00b6 We turn our attention next to traffic shifting features of a service mesh.","title":"Observability"},{"location":"dashboards/#observability","text":"This lab explores one of the main strengths of Istio: observability. The services in our mesh are automatically observable, without adding any burden on developers.","title":"Observability"},{"location":"dashboards/#deploy-the-addons","text":"The Istio distribution provides addons for a number of systems that together provide observability for the service mesh: Zipkin or Jaeger for distributed tracing Prometheus for metrics collection Grafana provides dashboards for monitoring, using Prometheus as the data source Kiali allows us to visualize the mesh These addons are located in the samples/addons/ folder of the distribution. Navigate to the addons directory cd ~/istio-1.18.2/samples/addons Deploy each addon: kubectl apply -f extras/zipkin.yaml kubectl apply -f prometheus.yaml kubectl apply -f grafana.yaml kubectl apply -f kiali.yaml Verify that the istio-system namespace is now running additional workloads for each of the addons. kubectl get pod -n istio-system The istioctl CLI provides convenience commands for accessing the web UIs for each dashboard. Take a moment to review the help information for the istioctl dashboard command: istioctl dashboard --help","title":"Deploy the Addons"},{"location":"dashboards/#generate-a-load","text":"In order to have something to observe, we need to generate a load on our system.","title":"Generate a load"},{"location":"dashboards/#install-a-load-generator","text":"Install a simple load generating tool named siege . We normally install siege with the apt-get package manager. However, given the cloud shell's ephemeral nature, anything installed outside our home directory will vanish after a session timeout. Alternatives: Install from source. It's a little more work, but does not exhibit the above-mentioned problem. Run the load generator from your laptop. On a mac, using homebrew the command is brew install siege . Use a simple bash curl command wrapped in a while loop: while true ; do curl $GATEWAY_IP | head ; sleep 1s ; done Here are the steps to install from source: Fetch the package wget http://download.joedog.org/siege/siege-latest.tar.gz Unpack it tar -xzf siege-latest.tar.gz Navigate into the siege subdirectory with cd siege Tab Run the configure script, and request that siege get installed inside your home directory ./configure --prefix = $HOME Build the code make Finally, install (copies the binary to ~/bin ) make install Feel free to delete (or preserve) the downloaded tar file and source code.","title":"Install a load generator"},{"location":"dashboards/#generate-a-load_1","text":"With siege now installed, familiarize yourself with the command and its options. siege --help Run the following command to generate a mild load against the application. siege --delay = 3 --concurrent = 3 http:// $GATEWAY_IP / Note The siege command stays in the foreground while it runs. It may be simplest to leave it running, and open a separate terminal in your cloud shell environment.","title":"Generate a load"},{"location":"dashboards/#kiali","text":"Launch the Kiali dashboard: istioctl dashboard kiali Warning If the dashboard page fails to open, just click on the hyperlink in the console output. Note The istioctl dashboard command also blocks. Leave it running until you're finished using the dashboard, at which time pressing Ctrl + C can interrupt the process and put you back at the terminal prompt. The Kiali dashboard displays. Customize the view as follows: Select the Graph section from the sidebar. Under Select Namespaces (at the top of the page), select the default namespace, the location where the application's pods are running. From the third \"pulldown\" menu, select App graph . From the Display \"pulldown\", toggle on Traffic Animation and Security . From the footer, toggle the legend so that it is visible. Take a moment to familiarize yourself with the legend. Observe the visualization and note the following: We can see traffic coming in through the ingress gateway to the web-frontend , and the subsequent calls from the web-frontend to the customers service The lines connecting the services are green, indicating healthy requests The small lock icon on each edge in the graph indicates that the traffic is secured with mutual TLS Such visualizations are helpful with understanding the flow of requests in the mesh, and with diagnosis. Feel free to spend more time exploring Kiali. We will revisit Kiali in a later lab to visualize traffic shifting such as when performing a blue-green or canary deployment.","title":"Kiali"},{"location":"dashboards/#kiali-cleanup","text":"Close the Kiali dashboard. Interrupt the istioctl dashboard kiali command by pressing Ctrl + C .","title":"Kiali Cleanup"},{"location":"dashboards/#zipkin","text":"Launch the Zipkin dashboard: istioctl dashboard zipkin The Zipkin dashboard displays. Click on the red '+' button and select serviceName . Select the service named web-frontend.default and click on the Run Query button (lightblue) to the right. A number of query results will display. Each row is expandable and will display more detail in terms of the services participating in that particular trace. Click the Show button to the right of one of the traces having four (4) spans. The resulting view shows spans that are part of the trace, and more importantly how much time was spent within each span. Such information can help diagnose slow requests and pin-point where the latency lies. Distributed tracing also helps us make sense of the flow of requests in a microservice architecture.","title":"Zipkin"},{"location":"dashboards/#zipkin-cleanup","text":"Close the Zipkin dashboard. Interrupt the istioctl dashboard zipkin command with Ctrl + C .","title":"Zipkin Cleanup"},{"location":"dashboards/#prometheus","text":"Prometheus works by periodically calling a metrics endpoint against each running service (this endpoint is termed the \"scrape\" endpoint). Developers normally have to instrument their applications to expose such an endpoint and return metrics information in the format the Prometheus expects. With Istio, this is done automatically by the Envoy sidecar.","title":"Prometheus"},{"location":"dashboards/#observe-how-envoy-exposes-a-prometheus-scrape-endpoint","text":"Run the following command: kubectl exec svc/customers -- curl -s localhost:15020/stats/prometheus | grep istio_requests Why port 15020? See Ports used by Istio sidecar proxy. The list of metrics returned by the endpoint is rather lengthy, so we just peek at \"istio_requests\" metric. The full response contains many more metrics.","title":"Observe how Envoy exposes a Prometheus scrape endpoint"},{"location":"dashboards/#access-the-dashboard","text":"Start the prometheus dashboard istioctl dashboard prometheus In the search field enter the metric named istio_requests_total , and click the Execute button (on the right). Select the tab named Graph to obtain a graphical representation of this metric over time. Note that you are looking at requests across the entire mesh, i.e. this includes both requests to web-frontend and to customers . As an example of Prometheus' dimensional metrics capability, we can ask for total requests having a response code of 200: istio_requests_total{response_code=\"200\"} With respect to requests, it's more interesting to look at the rate of incoming requests over a time window. Try: rate(istio_requests_total[5m]) There's much more to the Prometheus query language ( this may be a good place to start). Grafana consumes these metrics to produce graphs on our behalf. Close the Prometheus dashboard and terminate the corresponding istioctl dashboard command.","title":"Access the dashboard"},{"location":"dashboards/#grafana","text":"Launch the Grafana dashboard istioctl dashboard grafana From the sidebar, select Dashboards \u2192 Browse Click on the folder named Istio to reveal pre-designed Istio-specific Grafana dashboards Explore the Istio Mesh Dashboard. Note the Global Request Volume and Global Success Rate. Explore the Istio Service Dashboard. First select the service web-frontend and inspect its metrics, then switch to the customers service and review its dashboard. Explore the Istio Workload Dashboard. Select the web-frontend workload. Look at Outbound Services and note the outbound requests to the customers service. Select the customers workload and note that it makes no Oubtound Services calls. Feel free to further explore these dashboards.","title":"Grafana"},{"location":"dashboards/#cleanup","text":"Terminate the istioctl dashboard command ( Ctrl + C ) Likewise, terminate the siege command","title":"Cleanup"},{"location":"dashboards/#next","text":"We turn our attention next to traffic shifting features of a service mesh.","title":"Next"},{"location":"discovery/","text":"Service discovery and load balancing \u00b6 This lab explores service discovery and load balancing in Istio. Clusters and endpoints \u00b6 The istioctl CLI's diagnostic command proxy-status provides a simple way to list all proxies that Istio knows about. Confirm that istiod knows about the workloads running on Kubernetes: istioctl proxy-status Deploy the helloworld sample \u00b6 The Istio distribution comes with a sample application \"helloworld\". cd ~/istio-1.18.2 Deploy helloworld to the default namespace. kubectl apply -f samples/helloworld/helloworld.yaml Check the output of proxy-status again, and confirm that helloworld is listed: istioctl proxy-status The service registry \u00b6 Istio maintains an internal service registry which can be observed through a debug endpoint /debug/registryz exposed by istiod : curl the registry endpoint: kubectl exec -n istio-system deploy/istiod -- curl localhost:15014/debug/registryz The output can be prettified with a tool such as jq . kubectl exec -n istio-system deploy/istiod -- curl localhost:15014/debug/registryz | jq . [] .hostname The sidecar configuration \u00b6 Review the deployments in the default namespace: kubectl get deploy The istioctl CLI's diagnostic command proxy-config will help us inspect the configuration of proxies. Envoy's term for a service is \"cluster\". Confirm that sleep knows about other services ( helloworld , customers , etc..): istioctl proxy-config clusters deploy/sleep List the endpoints backing each \"cluster\": istioctl proxy-config endpoints deploy/sleep Zero in on the endpoints for the helloworld service: istioctl proxy-config endpoints deploy/sleep \\ --cluster \"outbound|5000||helloworld.default.svc.cluster.local\" Load balancing \u00b6 The sleep pod's container image has curl pre-installed. Make repeated calls to the helloworld service from the sleep pod: kubectl exec deploy/sleep -- curl -s helloworld:5000/hello Some responses will be from helloworld-v1 while others from helloworld-v2 , an indication that Envoy is load-balancing requests between these two endpoints.","title":"Service discovery and load balancing"},{"location":"discovery/#service-discovery-and-load-balancing","text":"This lab explores service discovery and load balancing in Istio.","title":"Service discovery and load balancing"},{"location":"discovery/#clusters-and-endpoints","text":"The istioctl CLI's diagnostic command proxy-status provides a simple way to list all proxies that Istio knows about. Confirm that istiod knows about the workloads running on Kubernetes: istioctl proxy-status","title":"Clusters and endpoints"},{"location":"discovery/#deploy-the-helloworld-sample","text":"The Istio distribution comes with a sample application \"helloworld\". cd ~/istio-1.18.2 Deploy helloworld to the default namespace. kubectl apply -f samples/helloworld/helloworld.yaml Check the output of proxy-status again, and confirm that helloworld is listed: istioctl proxy-status","title":"Deploy the helloworld sample"},{"location":"discovery/#the-service-registry","text":"Istio maintains an internal service registry which can be observed through a debug endpoint /debug/registryz exposed by istiod : curl the registry endpoint: kubectl exec -n istio-system deploy/istiod -- curl localhost:15014/debug/registryz The output can be prettified with a tool such as jq . kubectl exec -n istio-system deploy/istiod -- curl localhost:15014/debug/registryz | jq . [] .hostname","title":"The service registry"},{"location":"discovery/#the-sidecar-configuration","text":"Review the deployments in the default namespace: kubectl get deploy The istioctl CLI's diagnostic command proxy-config will help us inspect the configuration of proxies. Envoy's term for a service is \"cluster\". Confirm that sleep knows about other services ( helloworld , customers , etc..): istioctl proxy-config clusters deploy/sleep List the endpoints backing each \"cluster\": istioctl proxy-config endpoints deploy/sleep Zero in on the endpoints for the helloworld service: istioctl proxy-config endpoints deploy/sleep \\ --cluster \"outbound|5000||helloworld.default.svc.cluster.local\"","title":"The sidecar configuration"},{"location":"discovery/#load-balancing","text":"The sleep pod's container image has curl pre-installed. Make repeated calls to the helloworld service from the sleep pod: kubectl exec deploy/sleep -- curl -s helloworld:5000/hello Some responses will be from helloworld-v1 while others from helloworld-v2 , an indication that Envoy is load-balancing requests between these two endpoints.","title":"Load balancing"},{"location":"environment/","text":"Lab environment \u00b6 Options \u00b6 If you brought your own Kubernetes cluster: Kubernetes versions 1.16 through 1.25 should all work. Feel free to consult the Istio support status of Istio releases page for version 1.18.2. We recommend a 3-worker node cluster of machine type \"e2-standard-2\" or similar, though a smaller cluster will likely work just fine. If you have your own public cloud account: On GCP, the following command should provision a GKE cluster of adequate size for the workshop: gcloud container clusters create my-istio-cluster \\ --cluster-version latest \\ --machine-type \"e2-standard-2\" \\ --num-nodes \"3\" \\ --network \"default\" Feel free to provision a K8S cluster on any infrastructure of your choosing. If you received Google credentials from the workshop instructors: A Kubernetes cluster has already been provisioned for you. Your instructor will demonstrate the process of accessing and configuring your environment, described below. The instructions below explain in detail how to access your account, select your project, and launch the cloud shell. If you are bringing your own Kubernetes cluster, please skip ahead to the artifacts section at the bottom of this page. Log in to GCP \u00b6 Log in to GCP using credentials provided by your instructor. Agree to the terms You will be prompted to select your country, click \"Agree and continue\" Select your project \u00b6 Select the GCP project you have been assigned, as follows: Click the project selector \"pulldown\" menu from the top banner, which will open a popup dialog Make sure the Select from organization is set to tetratelabs.com Select the tab named All You will see your GCP project name listed under the organization tetratelabs.com Select the project from the list Verify that your project is selected: If you look in the banner now, you will see your selected project displayed. Launch the Cloud Shell \u00b6 The Google Cloud Shell will serve as your terminal environment for these labs. Click the Activate cloud shell icon (top right); the icon looks like this: A dialog may pop up, click Continue Your cloud shell terminal should appear at the bottom of the screen Feel free to expand the size of the cloud shell, or even open it in a separate window (locate the icon button in the terminal header, on the right) Warning Your connection to the Cloud Shell gets severed after a period of inactivity. Click on the Reconnect button when this happens. Configure cluster access \u00b6 Check that the kubectl CLI is installed kubectl version --short Generate a kubeconfig entry With the user interface From the command line Activate the top navigation menu ( Menu icon on the top left hand side of the page) Locate and click on the product Kubernetes Engine (you may have to scroll down until you see it) Your pre-provisioned 3-node Kubernetes cluster should appear in the main view Click on that row's \"three dot\" menu and select the Connect option A dialog prompt will appear with instructions Copy the gcloud command shown and paste it in your cloud shell gcloud container clusters get-credentials \\ $( gcloud container clusters list --format = \"value(name)\" ) \\ --zone $( gcloud container clusters list --format = \"value(location)\" ) \\ --project $( gcloud config get-value project ) Click Authorize when prompted The console message will state that a kubeconfig entry [was] generated for [your project] Verify that your Kubernetes context is set for your cluster kubectl config get-contexts Run a token command such as kubectl get node or kubectl get ns to ensure that you can communicate with the Kubernetes API Server. kubectl get ns Tip This workshop makes extensive use of the kubectl CLI. Consider configuring an alias to make typing a little easier. cat << EOF >> ~/.bashrc source <(kubectl completion bash) alias k=kubectl complete -F __start_kubectl k EOF source ~/.bashrc All instructions in subsequent labs assume you will be working from the Google Cloud Shell. Artifacts \u00b6 The lab instructions reference Kubernetes yaml artifacts that you will need to apply to your cluster at specific points in time. You have the option of copying and pasting the yaml snippets directly from the lab instructions as you encounter them. Another option is to clone the GitHub repository for this workshop from the Cloud Shell. You will find all yaml artifacts in the subdirectory named artifacts . git clone https://github.com/tetratelabs/qcon-labs.git && \\ mv qcon-labs/artifacts . && \\ rm -rf qcon-labs Next \u00b6 Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.","title":"Lab environment"},{"location":"environment/#lab-environment","text":"","title":"Lab environment"},{"location":"environment/#options","text":"If you brought your own Kubernetes cluster: Kubernetes versions 1.16 through 1.25 should all work. Feel free to consult the Istio support status of Istio releases page for version 1.18.2. We recommend a 3-worker node cluster of machine type \"e2-standard-2\" or similar, though a smaller cluster will likely work just fine. If you have your own public cloud account: On GCP, the following command should provision a GKE cluster of adequate size for the workshop: gcloud container clusters create my-istio-cluster \\ --cluster-version latest \\ --machine-type \"e2-standard-2\" \\ --num-nodes \"3\" \\ --network \"default\" Feel free to provision a K8S cluster on any infrastructure of your choosing. If you received Google credentials from the workshop instructors: A Kubernetes cluster has already been provisioned for you. Your instructor will demonstrate the process of accessing and configuring your environment, described below. The instructions below explain in detail how to access your account, select your project, and launch the cloud shell. If you are bringing your own Kubernetes cluster, please skip ahead to the artifacts section at the bottom of this page.","title":"Options"},{"location":"environment/#log-in-to-gcp","text":"Log in to GCP using credentials provided by your instructor. Agree to the terms You will be prompted to select your country, click \"Agree and continue\"","title":"Log in to GCP"},{"location":"environment/#select-your-project","text":"Select the GCP project you have been assigned, as follows: Click the project selector \"pulldown\" menu from the top banner, which will open a popup dialog Make sure the Select from organization is set to tetratelabs.com Select the tab named All You will see your GCP project name listed under the organization tetratelabs.com Select the project from the list Verify that your project is selected: If you look in the banner now, you will see your selected project displayed.","title":"Select your project"},{"location":"environment/#launch-the-cloud-shell","text":"The Google Cloud Shell will serve as your terminal environment for these labs. Click the Activate cloud shell icon (top right); the icon looks like this: A dialog may pop up, click Continue Your cloud shell terminal should appear at the bottom of the screen Feel free to expand the size of the cloud shell, or even open it in a separate window (locate the icon button in the terminal header, on the right) Warning Your connection to the Cloud Shell gets severed after a period of inactivity. Click on the Reconnect button when this happens.","title":"Launch the Cloud Shell"},{"location":"environment/#configure-cluster-access","text":"Check that the kubectl CLI is installed kubectl version --short Generate a kubeconfig entry With the user interface From the command line Activate the top navigation menu ( Menu icon on the top left hand side of the page) Locate and click on the product Kubernetes Engine (you may have to scroll down until you see it) Your pre-provisioned 3-node Kubernetes cluster should appear in the main view Click on that row's \"three dot\" menu and select the Connect option A dialog prompt will appear with instructions Copy the gcloud command shown and paste it in your cloud shell gcloud container clusters get-credentials \\ $( gcloud container clusters list --format = \"value(name)\" ) \\ --zone $( gcloud container clusters list --format = \"value(location)\" ) \\ --project $( gcloud config get-value project ) Click Authorize when prompted The console message will state that a kubeconfig entry [was] generated for [your project] Verify that your Kubernetes context is set for your cluster kubectl config get-contexts Run a token command such as kubectl get node or kubectl get ns to ensure that you can communicate with the Kubernetes API Server. kubectl get ns Tip This workshop makes extensive use of the kubectl CLI. Consider configuring an alias to make typing a little easier. cat << EOF >> ~/.bashrc source <(kubectl completion bash) alias k=kubectl complete -F __start_kubectl k EOF source ~/.bashrc All instructions in subsequent labs assume you will be working from the Google Cloud Shell.","title":"Configure cluster access"},{"location":"environment/#artifacts","text":"The lab instructions reference Kubernetes yaml artifacts that you will need to apply to your cluster at specific points in time. You have the option of copying and pasting the yaml snippets directly from the lab instructions as you encounter them. Another option is to clone the GitHub repository for this workshop from the Cloud Shell. You will find all yaml artifacts in the subdirectory named artifacts . git clone https://github.com/tetratelabs/qcon-labs.git && \\ mv qcon-labs/artifacts . && \\ rm -rf qcon-labs","title":"Artifacts"},{"location":"environment/#next","text":"Now that we have access to our environment and to our Kubernetes cluster, we can proceed to install Istio.","title":"Next"},{"location":"ingress/","text":"Ingress \u00b6 The objective of this lab is to expose the web-frontend service to the internet. The Ingress gateway \u00b6 When you installed Istio, in addition to deploying istiod to Kubernetes, the installation also provisioned an Ingress Gateway. View the corresponding Istio ingress gateway pod in the istio-system namespace. kubectl get pod -n istio-system A corresponding LoadBalancer type service was also created: kubectl get svc -n istio-system Make a note of the external IP address for the load balancer. Assign it to an environment variable. GATEWAY_IP = $( kubectl get svc -n istio-system istio-ingressgateway -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) A small investment When the cloud shell connection is severed, or when opening a new terminal tab, $GATEWAY_IP will no longer be in scope. Ensure GATEWAY_IP is set each time we start a new shell: cat << EOF >> ~/.bashrc export GATEWAY_IP=$(kubectl get svc -n istio-system istio-ingressgateway -ojsonpath='{.status.loadBalancer.ingress[0].ip}') EOF In normal circumstances we associate this IP address with a hostname via DNS. For the sake of simplicity, in this workshop we will use the gateway public IP address directly. Configuring ingress \u00b6 Configuring ingress with Istio is performed in two parts: Define a Gateway custom resource that governs the specific host, port, and protocol to expose Specify how requests should be routed with a VirtualService custom resource. Create a Gateway resource \u00b6 Review the following Gateway specification. gateway.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : frontend-gateway spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Above, we specify the HTTP protocol, port 80, and a wildcard (\"*\") host matcher which ensures that HTTP requests using the load balancer IP address $GATEWAY_IP will match. The selector istio: ingressgateway ensures that this gateway resource binds to the physical ingress gateway. Apply the gateway resource to your cluster. kubectl apply -f gateway.yaml Attempt an HTTP request in your browser to the gateway IP address. It should return a 404 (not found). Create a VirtualService resource \u00b6 Review the following VirtualService specification. web-frontend-virtualservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : web-frontend spec : hosts : - \"*\" gateways : - frontend-gateway http : - route : - destination : host : web-frontend.default.svc.cluster.local port : number : 80 Note how this specification references the name of the gateway (\"frontend-gateway\"), a matching host (\"*\"), and specifies a route for requests to be directed to the web-frontend service. Apply the virtual service resource to your cluster. kubectl apply -f web-frontend-virtualservice.yaml List virtual services in the default namespace. kubectl get virtualservice The output indicates that the virtual service named web-frontend is bound to the gateway, as well as any hostname that routes to the load balancer IP address. Finally, verify that you can now access web-frontend from your web browser using the gateway IP address. Candidate follow-on exercises \u00b6 We will not explore ingress any further in this workshop. Consider the following as independent exercises: Creating a DNS A record for the gateway IP, and narrowing down the scope of the gateway to only match that hostname. Configuring a TLS ingress gateway Next \u00b6 The application is now running and exposed on the internet. In the next lab, we turn our attention to the observability features that are built in to Istio.","title":"Ingress"},{"location":"ingress/#ingress","text":"The objective of this lab is to expose the web-frontend service to the internet.","title":"Ingress"},{"location":"ingress/#the-ingress-gateway","text":"When you installed Istio, in addition to deploying istiod to Kubernetes, the installation also provisioned an Ingress Gateway. View the corresponding Istio ingress gateway pod in the istio-system namespace. kubectl get pod -n istio-system A corresponding LoadBalancer type service was also created: kubectl get svc -n istio-system Make a note of the external IP address for the load balancer. Assign it to an environment variable. GATEWAY_IP = $( kubectl get svc -n istio-system istio-ingressgateway -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) A small investment When the cloud shell connection is severed, or when opening a new terminal tab, $GATEWAY_IP will no longer be in scope. Ensure GATEWAY_IP is set each time we start a new shell: cat << EOF >> ~/.bashrc export GATEWAY_IP=$(kubectl get svc -n istio-system istio-ingressgateway -ojsonpath='{.status.loadBalancer.ingress[0].ip}') EOF In normal circumstances we associate this IP address with a hostname via DNS. For the sake of simplicity, in this workshop we will use the gateway public IP address directly.","title":"The Ingress gateway"},{"location":"ingress/#configuring-ingress","text":"Configuring ingress with Istio is performed in two parts: Define a Gateway custom resource that governs the specific host, port, and protocol to expose Specify how requests should be routed with a VirtualService custom resource.","title":"Configuring ingress"},{"location":"ingress/#create-a-gateway-resource","text":"Review the following Gateway specification. gateway.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : frontend-gateway spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Above, we specify the HTTP protocol, port 80, and a wildcard (\"*\") host matcher which ensures that HTTP requests using the load balancer IP address $GATEWAY_IP will match. The selector istio: ingressgateway ensures that this gateway resource binds to the physical ingress gateway. Apply the gateway resource to your cluster. kubectl apply -f gateway.yaml Attempt an HTTP request in your browser to the gateway IP address. It should return a 404 (not found).","title":"Create a Gateway resource"},{"location":"ingress/#create-a-virtualservice-resource","text":"Review the following VirtualService specification. web-frontend-virtualservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : web-frontend spec : hosts : - \"*\" gateways : - frontend-gateway http : - route : - destination : host : web-frontend.default.svc.cluster.local port : number : 80 Note how this specification references the name of the gateway (\"frontend-gateway\"), a matching host (\"*\"), and specifies a route for requests to be directed to the web-frontend service. Apply the virtual service resource to your cluster. kubectl apply -f web-frontend-virtualservice.yaml List virtual services in the default namespace. kubectl get virtualservice The output indicates that the virtual service named web-frontend is bound to the gateway, as well as any hostname that routes to the load balancer IP address. Finally, verify that you can now access web-frontend from your web browser using the gateway IP address.","title":"Create a VirtualService resource"},{"location":"ingress/#candidate-follow-on-exercises","text":"We will not explore ingress any further in this workshop. Consider the following as independent exercises: Creating a DNS A record for the gateway IP, and narrowing down the scope of the gateway to only match that hostname. Configuring a TLS ingress gateway","title":"Candidate follow-on exercises"},{"location":"ingress/#next","text":"The application is now running and exposed on the internet. In the next lab, we turn our attention to the observability features that are built in to Istio.","title":"Next"},{"location":"install/","text":"Install Istio \u00b6 In this lab you will install Istio. Download Istio \u00b6 Run the following command from your home directory. curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .18.2 sh - Navigate into the directory created by the above command. cd istio-1.18.2 Add istioctl to your PATH \u00b6 The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , after your session times out that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH cat << EOF >> ~/.bashrc export PATH=\"~/bin:\\$PATH\" EOF And then: source ~/.bashrc Verify that istioctl is installed with: istioctl version The output should indicate that the version is 1.18.2. Turn on command completion for istioctl Create the folder to contain completion scripts: mkdir -p ~/.local/share/bash-completion/completions Generate the completion script: istioctl completion bash > .local/share/bash-completion/completions/istioctl Open a new shell to activate completion. This small investment will pay for itself in the course of this workshop. With the CLI installed, proceed to install Istio to Kubernetes. Pre-check \u00b6 The istioctl CLI provides a convenient precheck command that can be used to \" inspect a Kubernetes cluster for Istio install and upgrade requirements. \" To verify whether it is safe to install Istio on your Kubernetes cluster, run: istioctl x precheck Make sure that the output of the above command returns a green \"checkmark\" stating that no issues were found when checking the cluster. Install Istio \u00b6 Take a moment to learn about Istio installation profiles . Choose your journey: Simple Recipe Separate Gateway Recipe This installation recipe uses the default profile, which installs both istiod and istio-ingressgateway components in a single step. Istio can be installed directly with the CLI: istioctl install When prompted, enter y to proceed to install Istio. In production settings, it is preferrable to install the Istio gateway component separately, providing the flexibility of upgrading each component independently. This full procedure can be found in the Istio documentation . Install Istio by itself \u00b6 istioctl install --set profile = minimal Install the ingress gateway separately \u00b6 Review the ingress.yaml installation configuration (included in artifacts/ ) install/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- apiVersion : install.istio.io/v1alpha1 kind : IstioOperator metadata : name : ingress spec : profile : empty # Do not install CRDs or the control plane components : ingressGateways : - name : istio-ingressgateway namespace : istio-system enabled : true label : # Set a unique label for the gateway. This is required to ensure Gateways # can select this workload istio : ingressgateway values : gateways : istio-ingressgateway : # Enable gateway injection injectionTemplate : gateway Note: The use of the empty profile, with the ingress gateway component added \"a la carte.\" For simplicity, the gateway is installed to the istio-system namespace. The assignment of a label to the gateway deployment (so that later, we can define Gateway custom resources that target this component). Install the Gateway component istioctl install -f ingress.yaml Verify that Istio is installed \u00b6 Post-installation, Istio provides the command verify-install : it runs a series of checks to ensure that the installation was successful and complete. Go ahead and run it: istioctl verify-install Inspect the output and confirm that the it states that \" \u2714 Istio is installed and verified successfully. \" Keep probing: List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that Istio is indeed present in the cluster. Next \u00b6 With Istio installed, we are ready to deploy an application to the mesh.","title":"Install Istio"},{"location":"install/#install-istio","text":"In this lab you will install Istio.","title":"Install Istio"},{"location":"install/#download-istio","text":"Run the following command from your home directory. curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .18.2 sh - Navigate into the directory created by the above command. cd istio-1.18.2","title":"Download Istio"},{"location":"install/#add-istioctl-to-your-path","text":"The istioctl CLI is located in the bin/ subdirectory. Note Cloud Shell only preserves files located inside your home directory across sessions. This means that if you install a binary to a PATH such as /usr/local/bin , after your session times out that file will no longer be there! As a workaround, you will add ${HOME}/bin to your PATH and place the binary there. Create a bin subdirectory in your home directory: mkdir ~/bin Copy the CLI to that subdirectory: cp ./bin/istioctl ~/bin Add your home bin subdirectory to your PATH cat << EOF >> ~/.bashrc export PATH=\"~/bin:\\$PATH\" EOF And then: source ~/.bashrc Verify that istioctl is installed with: istioctl version The output should indicate that the version is 1.18.2. Turn on command completion for istioctl Create the folder to contain completion scripts: mkdir -p ~/.local/share/bash-completion/completions Generate the completion script: istioctl completion bash > .local/share/bash-completion/completions/istioctl Open a new shell to activate completion. This small investment will pay for itself in the course of this workshop. With the CLI installed, proceed to install Istio to Kubernetes.","title":"Add istioctl to your PATH"},{"location":"install/#pre-check","text":"The istioctl CLI provides a convenient precheck command that can be used to \" inspect a Kubernetes cluster for Istio install and upgrade requirements. \" To verify whether it is safe to install Istio on your Kubernetes cluster, run: istioctl x precheck Make sure that the output of the above command returns a green \"checkmark\" stating that no issues were found when checking the cluster.","title":"Pre-check"},{"location":"install/#install-istio_1","text":"Take a moment to learn about Istio installation profiles . Choose your journey: Simple Recipe Separate Gateway Recipe This installation recipe uses the default profile, which installs both istiod and istio-ingressgateway components in a single step. Istio can be installed directly with the CLI: istioctl install When prompted, enter y to proceed to install Istio. In production settings, it is preferrable to install the Istio gateway component separately, providing the flexibility of upgrading each component independently. This full procedure can be found in the Istio documentation .","title":"Install Istio"},{"location":"install/#install-istio-by-itself","text":"istioctl install --set profile = minimal","title":"Install Istio by itself"},{"location":"install/#install-the-ingress-gateway-separately","text":"Review the ingress.yaml installation configuration (included in artifacts/ ) install/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- apiVersion : install.istio.io/v1alpha1 kind : IstioOperator metadata : name : ingress spec : profile : empty # Do not install CRDs or the control plane components : ingressGateways : - name : istio-ingressgateway namespace : istio-system enabled : true label : # Set a unique label for the gateway. This is required to ensure Gateways # can select this workload istio : ingressgateway values : gateways : istio-ingressgateway : # Enable gateway injection injectionTemplate : gateway Note: The use of the empty profile, with the ingress gateway component added \"a la carte.\" For simplicity, the gateway is installed to the istio-system namespace. The assignment of a label to the gateway deployment (so that later, we can define Gateway custom resources that target this component). Install the Gateway component istioctl install -f ingress.yaml","title":"Install the ingress gateway separately"},{"location":"install/#verify-that-istio-is-installed","text":"Post-installation, Istio provides the command verify-install : it runs a series of checks to ensure that the installation was successful and complete. Go ahead and run it: istioctl verify-install Inspect the output and confirm that the it states that \" \u2714 Istio is installed and verified successfully. \" Keep probing: List Kubernetes namespaces and note the new namespace istio-system kubectl get ns Verify that the istiod controller pod is running in that namespace kubectl get pod -n istio-system Re-run istioctl version . The output should include a control plane version, indicating that Istio is indeed present in the cluster.","title":"Verify that Istio is installed"},{"location":"install/#next","text":"With Istio installed, we are ready to deploy an application to the mesh.","title":"Next"},{"location":"security/","text":"Security \u00b6 In this lab we explore some of the security features of the Istio service mesh. Mutual TLS \u00b6 By default, Istio is configured such that when a service is deployed onto the mesh, it will take advantage of mutual TLS: the service is given an identity as a function of its associated service account and namespace an x.509 certificate is issued to the workload (and regularly rotated) and used to identify the workload in calls to other services In the observability lab, we looked at the Kiali dashboard and noted the lock icons indicating that traffic was secured with mTLS. Can a workload receive plain-text requests? \u00b6 We can test whether a mesh workload, such as the customers service, will allow a plain-text request as follows: Create a separate namespace that is not configured with automatic injection. kubectl create ns other-ns Deploy sleep to that namespace kubectl apply -f sleep.yaml -n other-ns Verify that the sleep pod has no sidecars: kubectl get pod -n other-ns Call the customer service from that pod: kubectl exec -n other-ns deploy/sleep -- curl -s customers.default The output should look like a list of customers in JSON format. We conclude that Istio is configured by default to allow plain-text request. This is called permissive mode and is specifically designed to allow services that have not yet fully onboarded onto the mesh to participate. Challenge Above, you were asked to create a namespace without marking it for sidecar injection in order to effectively have a client that can call other services without employing mutual TLS. Can you find another way of achieving the same result? Can a client be configured explicitly to not use mTLS? Enable strict mode \u00b6 Istio provides the PeerAuthentication custom resource to define peer authentication policy. Apply the following peer authentication policy. mtls-strict.yaml 1 2 3 4 5 6 7 8 9 --- apiVersion : security.istio.io/v1beta1 kind : PeerAuthentication metadata : name : default namespace : default spec : mtls : mode : STRICT Info Strict mtls can be enabled globally by setting the namespace to the name of the Istio root namespace, which by default is istio-system Verify that the peer authentication has been applied. kubectl get peerauthentication Verify that plain-text requests are no longer permitted \u00b6 kubectl exec -n other-ns deploy/sleep -- curl customers.default The console output should indicate that the connection was reset by peer . Security in depth \u00b6 Another important layer of security is to define an authorization policy, in which we allow only specific services to communicate with other services. At the moment, any container can, for example, call the customers service or the web-frontend service. Call the customers service. kubectl exec deploy/sleep -- curl -s customers Call the web-frontend service. kubectl exec deploy/sleep -- curl -s web-frontend | head Both calls succeed. We wish to apply a policy in which only web-frontend is allowed to call customers , and only the ingress gateway can call web-frontend . Study the below authorization policy. authz-policy-customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowed-customers-clients namespace : default spec : selector : matchLabels : app : customers action : ALLOW rules : - from : - source : principals : [ \"cluster.local/ns/default/sa/web-frontend\" ] The selector section specifies that the policy applies to the customers service. Note how the rules have a \"from: source: \" section indicating who is allowed in. The nomenclature for the value of the principals field comes from the spiffe standard. Note how it captures the service account name and namespace associated with the web-frontend service. This identify is associated with the x.509 certificate used by each service when making secure mtls calls to one another. Tasks: Apply the policy to your cluster. Verify that you are no longer able to reach the customers pod from the sleep pod Challenge \u00b6 Can you come up with a similar authorization policy for web-frontend ? Use a copy of the customers authorization policy as a starting point Give the resource an apt name Revise the selector to match the web-frontend service Revise the rule to match the principal of the ingress gateway Hint The ingress gateway has its own identity. Here is a command which can help you find the name of the service account associated with its identity: kubectl get pod -n istio-system -l app = istio-ingressgateway -o yaml | grep serviceAccountName Use this service account name together with the namespace that the ingress gateway is running in to specify the value for the principals field. Test it \u00b6 Don't forget to verify that the policy is enforced. Call both services again from the sleep pod and ensure communication is no longer allowed. The console output should contain the message RBAC: access denied . Cleanup \u00b6 To prevent the above authorization policies from getting in the way of further exploration in subsequent labs, delete them: kubectl delete authorizationpolicies --all Next \u00b6 In the next lab we show how to use Istio's traffic management features to upgrade the customers service with zero downtime.","title":"Security"},{"location":"security/#security","text":"In this lab we explore some of the security features of the Istio service mesh.","title":"Security"},{"location":"security/#mutual-tls","text":"By default, Istio is configured such that when a service is deployed onto the mesh, it will take advantage of mutual TLS: the service is given an identity as a function of its associated service account and namespace an x.509 certificate is issued to the workload (and regularly rotated) and used to identify the workload in calls to other services In the observability lab, we looked at the Kiali dashboard and noted the lock icons indicating that traffic was secured with mTLS.","title":"Mutual TLS"},{"location":"security/#can-a-workload-receive-plain-text-requests","text":"We can test whether a mesh workload, such as the customers service, will allow a plain-text request as follows: Create a separate namespace that is not configured with automatic injection. kubectl create ns other-ns Deploy sleep to that namespace kubectl apply -f sleep.yaml -n other-ns Verify that the sleep pod has no sidecars: kubectl get pod -n other-ns Call the customer service from that pod: kubectl exec -n other-ns deploy/sleep -- curl -s customers.default The output should look like a list of customers in JSON format. We conclude that Istio is configured by default to allow plain-text request. This is called permissive mode and is specifically designed to allow services that have not yet fully onboarded onto the mesh to participate. Challenge Above, you were asked to create a namespace without marking it for sidecar injection in order to effectively have a client that can call other services without employing mutual TLS. Can you find another way of achieving the same result? Can a client be configured explicitly to not use mTLS?","title":"Can a workload receive plain-text requests?"},{"location":"security/#enable-strict-mode","text":"Istio provides the PeerAuthentication custom resource to define peer authentication policy. Apply the following peer authentication policy. mtls-strict.yaml 1 2 3 4 5 6 7 8 9 --- apiVersion : security.istio.io/v1beta1 kind : PeerAuthentication metadata : name : default namespace : default spec : mtls : mode : STRICT Info Strict mtls can be enabled globally by setting the namespace to the name of the Istio root namespace, which by default is istio-system Verify that the peer authentication has been applied. kubectl get peerauthentication","title":"Enable strict mode"},{"location":"security/#verify-that-plain-text-requests-are-no-longer-permitted","text":"kubectl exec -n other-ns deploy/sleep -- curl customers.default The console output should indicate that the connection was reset by peer .","title":"Verify that plain-text requests are no longer permitted"},{"location":"security/#security-in-depth","text":"Another important layer of security is to define an authorization policy, in which we allow only specific services to communicate with other services. At the moment, any container can, for example, call the customers service or the web-frontend service. Call the customers service. kubectl exec deploy/sleep -- curl -s customers Call the web-frontend service. kubectl exec deploy/sleep -- curl -s web-frontend | head Both calls succeed. We wish to apply a policy in which only web-frontend is allowed to call customers , and only the ingress gateway can call web-frontend . Study the below authorization policy. authz-policy-customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowed-customers-clients namespace : default spec : selector : matchLabels : app : customers action : ALLOW rules : - from : - source : principals : [ \"cluster.local/ns/default/sa/web-frontend\" ] The selector section specifies that the policy applies to the customers service. Note how the rules have a \"from: source: \" section indicating who is allowed in. The nomenclature for the value of the principals field comes from the spiffe standard. Note how it captures the service account name and namespace associated with the web-frontend service. This identify is associated with the x.509 certificate used by each service when making secure mtls calls to one another. Tasks: Apply the policy to your cluster. Verify that you are no longer able to reach the customers pod from the sleep pod","title":"Security in depth"},{"location":"security/#challenge","text":"Can you come up with a similar authorization policy for web-frontend ? Use a copy of the customers authorization policy as a starting point Give the resource an apt name Revise the selector to match the web-frontend service Revise the rule to match the principal of the ingress gateway Hint The ingress gateway has its own identity. Here is a command which can help you find the name of the service account associated with its identity: kubectl get pod -n istio-system -l app = istio-ingressgateway -o yaml | grep serviceAccountName Use this service account name together with the namespace that the ingress gateway is running in to specify the value for the principals field.","title":"Challenge"},{"location":"security/#test-it","text":"Don't forget to verify that the policy is enforced. Call both services again from the sleep pod and ensure communication is no longer allowed. The console output should contain the message RBAC: access denied .","title":"Test it"},{"location":"security/#cleanup","text":"To prevent the above authorization policies from getting in the way of further exploration in subsequent labs, delete them: kubectl delete authorizationpolicies --all","title":"Cleanup"},{"location":"security/#next","text":"In the next lab we show how to use Istio's traffic management features to upgrade the customers service with zero downtime.","title":"Next"},{"location":"sidecar-injection/","text":"Sidecar injection \u00b6 This lab explores sidecar injection in Istio. Preface \u00b6 Istio provides both a manual and an automatic mechanism for injecting sidecars alongside workloads. In this lab you will use the manual method, because it provides the opportunity to inspect the transformed deployment manifest even before applying it to a target Kubernetes cluster. You will learn about automatic sidecar injection in the next lab. Generate a Pod spec \u00b6 The kubectl command's dry-run flag provides a simple way to generate and capture a simple pod specification. Generate a Pod spec for a simple web server, as follows: kubectl run mywebserver --image nginx \\ --dry-run = client -oyaml > nginx-pod.yaml Inspect the contents of the generated file. Here it is below, slightly cleaned up: nginx-pod.yaml 1 2 3 4 5 6 7 8 9 10 11 --- apiVersion : v1 kind : Pod metadata : labels : run : mywebserver name : mywebserver spec : containers : - name : mywebserver image : nginx The main thing to note at this point is that this Pod spec consists of a single container using the image nginx . Transform the Pod spec \u00b6 The istioctl command provides the convenient kube-inject subcommand, that can transform such a specification into one that includes the necessary sidecar. Learn the kube-inject command's usage: istioctl kube-inject --help Use the command to generate and capture the full sidecar-injected manifest to a new file named transformed.yaml . Show me how istioctl kube-inject --filename ./nginx-pod.yaml > transformed.yaml Study the sidecar container specification \u00b6 The modified Pod specification now includes a second container. Here is the salient part: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 - name : istio-proxy image : docker.io/istio/proxyv2:1.18.2 args : - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --log_output_level=default:info - --concurrency - \"2\" env : - ... The container name is istio-proxy and the docker image is istio/proxyv2 . What command is actually run? To find out what command actually runs inside that container, we can inspect the docker container specification and view the Entrypoint field: docker pull docker.io/istio/proxyv2:1.18.2 docker inspect istio/proxyv2:1.18.2 | grep Entrypoint -A 2 Here is the output: \"Entrypoint\" : [ \"/usr/local/bin/pilot-agent\" ], We learn that the name of the command is pilot-agent . By extracting the arguments from the yaml, we can reconstitute the full command executed inside the sidecar container: pilot-agent proxy sidecar \\ --domain $( POD_NAMESPACE ) .svc.cluster.local \\ --proxyLogLevel = warning \\ --proxyComponentLogLevel = misc:error \\ --log_output_level = default:info \\ --concurrency \"2\" Apply the manifest \u00b6 Deploy the transformed manifest to Kubernetes: kubectl apply -f transformed.yaml List pods in the default namespace kubectl get pod Once the pod reaches Running state, note the READY column in the output displays 2 out of 2 containers: NAME READY STATUS RESTARTS AGE mywebserver 2/2 Running 0 36s Study the running processes \u00b6 Run the ps command from inside the sidecar container, like so: kubectl exec mywebserver -c istio-proxy -- ps -ef Here is the output, slightly cleaned up, showing both the pilot-agent process, and the envoy process that it bootstrapped: PID PPID CMD 1 0 /usr/local/bin/pilot-agent proxy sidecar --domain ... 16 1 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json ... We can learn more about the pilot-agent command by running pilot-agent --help from inside the sidecar container: kubectl exec mywebserver -c istio-proxy -- pilot-agent --help Study the initContainers specification \u00b6 Besides injecting a sidecar container, the transformation operation also adds an initContainers section. Here is the relevant section: initContainers : - name : istio-init image : docker.io/istio/proxyv2:1.18.2 args : - istio-iptables - -p - \"15001\" - -z - \"15006\" - -u - \"1337\" - -m - REDIRECT - -i - '*' - -x - \"\" - -b - '*' - -d - 15090,15021,15020 - --log_output_level=default:info The \"initContainer\" uses the same image as the sidecar container: istio/proxyv2 . The difference lies in the command that is run when the Pod initializes. Here is the reconstituted command with long-form versions of each option, to clarify the instruction: pilot-agent istio-iptables \\ --envoy-port \"15001\" \\ --inbound-capture-port \"15006\" \\ --proxy-uid \"1337\" \\ --istio-inbound-interception-mode REDIRECT \\ --istio-service-cidr '*' \\ --istio-service-exclude-cidr \"\" \\ --istio-inbound-ports '*' \\ --istio-local-exclude-ports 15090 ,15021,15020 \\ --log_output_level = default:info Tip For a full description of the istio-iptables subcommand and its options, run: kubectl exec mywebserver -c istio-proxy -- pilot-agent istio-iptables --help The gist of the command is that, through iptables rules, the routing of network packets inside the Pod is reconfigured to give Envoy the chance to intercept and proxy inbound and outbound traffic. We need not concern ourselves with the specific port numbers, exclusions, and other low-level details at this time. The lesson of this exercise is to learn how to get at these details. Going forward.. \u00b6 The above process of transforming a deployment manifest on its way to the Kube API Server is streamlined when using automatic sidecar injection. The next lab will walk you through how automatic sidecar injection is accomplished. From here on, we will use automatic sidecar injection when deploying workloads to the mesh. Cleanup \u00b6 kubectl delete -f transformed.yaml","title":"Sidecar injection"},{"location":"sidecar-injection/#sidecar-injection","text":"This lab explores sidecar injection in Istio.","title":"Sidecar injection"},{"location":"sidecar-injection/#preface","text":"Istio provides both a manual and an automatic mechanism for injecting sidecars alongside workloads. In this lab you will use the manual method, because it provides the opportunity to inspect the transformed deployment manifest even before applying it to a target Kubernetes cluster. You will learn about automatic sidecar injection in the next lab.","title":"Preface"},{"location":"sidecar-injection/#generate-a-pod-spec","text":"The kubectl command's dry-run flag provides a simple way to generate and capture a simple pod specification. Generate a Pod spec for a simple web server, as follows: kubectl run mywebserver --image nginx \\ --dry-run = client -oyaml > nginx-pod.yaml Inspect the contents of the generated file. Here it is below, slightly cleaned up: nginx-pod.yaml 1 2 3 4 5 6 7 8 9 10 11 --- apiVersion : v1 kind : Pod metadata : labels : run : mywebserver name : mywebserver spec : containers : - name : mywebserver image : nginx The main thing to note at this point is that this Pod spec consists of a single container using the image nginx .","title":"Generate a Pod spec"},{"location":"sidecar-injection/#transform-the-pod-spec","text":"The istioctl command provides the convenient kube-inject subcommand, that can transform such a specification into one that includes the necessary sidecar. Learn the kube-inject command's usage: istioctl kube-inject --help Use the command to generate and capture the full sidecar-injected manifest to a new file named transformed.yaml . Show me how istioctl kube-inject --filename ./nginx-pod.yaml > transformed.yaml","title":"Transform the Pod spec"},{"location":"sidecar-injection/#study-the-sidecar-container-specification","text":"The modified Pod specification now includes a second container. Here is the salient part: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 - name : istio-proxy image : docker.io/istio/proxyv2:1.18.2 args : - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --log_output_level=default:info - --concurrency - \"2\" env : - ... The container name is istio-proxy and the docker image is istio/proxyv2 . What command is actually run? To find out what command actually runs inside that container, we can inspect the docker container specification and view the Entrypoint field: docker pull docker.io/istio/proxyv2:1.18.2 docker inspect istio/proxyv2:1.18.2 | grep Entrypoint -A 2 Here is the output: \"Entrypoint\" : [ \"/usr/local/bin/pilot-agent\" ], We learn that the name of the command is pilot-agent . By extracting the arguments from the yaml, we can reconstitute the full command executed inside the sidecar container: pilot-agent proxy sidecar \\ --domain $( POD_NAMESPACE ) .svc.cluster.local \\ --proxyLogLevel = warning \\ --proxyComponentLogLevel = misc:error \\ --log_output_level = default:info \\ --concurrency \"2\"","title":"Study the sidecar container specification"},{"location":"sidecar-injection/#apply-the-manifest","text":"Deploy the transformed manifest to Kubernetes: kubectl apply -f transformed.yaml List pods in the default namespace kubectl get pod Once the pod reaches Running state, note the READY column in the output displays 2 out of 2 containers: NAME READY STATUS RESTARTS AGE mywebserver 2/2 Running 0 36s","title":"Apply the manifest"},{"location":"sidecar-injection/#study-the-running-processes","text":"Run the ps command from inside the sidecar container, like so: kubectl exec mywebserver -c istio-proxy -- ps -ef Here is the output, slightly cleaned up, showing both the pilot-agent process, and the envoy process that it bootstrapped: PID PPID CMD 1 0 /usr/local/bin/pilot-agent proxy sidecar --domain ... 16 1 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev.json ... We can learn more about the pilot-agent command by running pilot-agent --help from inside the sidecar container: kubectl exec mywebserver -c istio-proxy -- pilot-agent --help","title":"Study the running processes"},{"location":"sidecar-injection/#study-the-initcontainers-specification","text":"Besides injecting a sidecar container, the transformation operation also adds an initContainers section. Here is the relevant section: initContainers : - name : istio-init image : docker.io/istio/proxyv2:1.18.2 args : - istio-iptables - -p - \"15001\" - -z - \"15006\" - -u - \"1337\" - -m - REDIRECT - -i - '*' - -x - \"\" - -b - '*' - -d - 15090,15021,15020 - --log_output_level=default:info The \"initContainer\" uses the same image as the sidecar container: istio/proxyv2 . The difference lies in the command that is run when the Pod initializes. Here is the reconstituted command with long-form versions of each option, to clarify the instruction: pilot-agent istio-iptables \\ --envoy-port \"15001\" \\ --inbound-capture-port \"15006\" \\ --proxy-uid \"1337\" \\ --istio-inbound-interception-mode REDIRECT \\ --istio-service-cidr '*' \\ --istio-service-exclude-cidr \"\" \\ --istio-inbound-ports '*' \\ --istio-local-exclude-ports 15090 ,15021,15020 \\ --log_output_level = default:info Tip For a full description of the istio-iptables subcommand and its options, run: kubectl exec mywebserver -c istio-proxy -- pilot-agent istio-iptables --help The gist of the command is that, through iptables rules, the routing of network packets inside the Pod is reconfigured to give Envoy the chance to intercept and proxy inbound and outbound traffic. We need not concern ourselves with the specific port numbers, exclusions, and other low-level details at this time. The lesson of this exercise is to learn how to get at these details.","title":"Study the initContainers specification"},{"location":"sidecar-injection/#going-forward","text":"The above process of transforming a deployment manifest on its way to the Kube API Server is streamlined when using automatic sidecar injection. The next lab will walk you through how automatic sidecar injection is accomplished. From here on, we will use automatic sidecar injection when deploying workloads to the mesh.","title":"Going forward.."},{"location":"sidecar-injection/#cleanup","text":"kubectl delete -f transformed.yaml","title":"Cleanup"},{"location":"summary/","text":"Congratulations \u00b6 Well-done on making it all the way to the end of the workshop! In this workshop, you have covered a lot of ground! Let's summarize. You have: Learned about Istio service mesh architecture Learned how to split traffic, use gateways and circuit breaker feature Learned how to use Istio to secure your service mesh and how access control works Learned how to use Istio to monitor your service mesh and how to use distributed tracing Learned how to onboard a workload running on a virtual machine You might also be interested in the free courses offered at the Tetrate Academy , including Istio Fundamentals , and Envoy Fundamentals . Finally, if you're interested in Istio certification, check out the Istio Certified Administrator exam. Thanks!","title":"Summary"},{"location":"summary/#congratulations","text":"Well-done on making it all the way to the end of the workshop! In this workshop, you have covered a lot of ground! Let's summarize. You have: Learned about Istio service mesh architecture Learned how to split traffic, use gateways and circuit breaker feature Learned how to use Istio to secure your service mesh and how access control works Learned how to use Istio to monitor your service mesh and how to use distributed tracing Learned how to onboard a workload running on a virtual machine You might also be interested in the free courses offered at the Tetrate Academy , including Istio Fundamentals , and Envoy Fundamentals . Finally, if you're interested in Istio certification, check out the Istio Certified Administrator exam. Thanks!","title":"Congratulations"},{"location":"the-app/","text":"The application \u00b6 In this lab you will deploy an application to your mesh. The application consists of two microservices, web-frontend and customers . Info The official Istio docs canonical example is the BookInfo application . For this workshop we felt that an application involving fewer microservices would be more clear. The customers service exposes a REST endpoint that returns a list of customers in JSON format. The web-frontend calls customers to retrieve the list, which it uses to render to HTML. The respective Docker images for these services have already been built and pushed to a Docker registry. You will deploy the application to the default Kubernetes namespace. But before proceeding, we must enable sidecar injection. Enable automatic sidecar injection \u00b6 There are two options for sidecar injection : automatic and manual. In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside. Label the default namespace kubectl label namespace default istio-injection = enabled Verify that the label has been applied: kubectl get ns -Listio-injection Deploy the application \u00b6 Study the two Kubernetes yaml files: web-frontend.yaml and customers.yaml . web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : web-frontend --- apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend labels : app : web-frontend spec : replicas : 1 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : \"http://customers.default.svc.cluster.local\" --- kind : Service apiVersion : v1 metadata : name : web-frontend labels : app : web-frontend spec : selector : app : web-frontend ports : - port : 80 name : http targetPort : 8080 customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- apiVersion : v1 kind : ServiceAccount metadata : name : customers --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v1 labels : app : customers version : v1 spec : replicas : 1 selector : matchLabels : app : customers version : v1 template : metadata : labels : app : customers version : v1 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : customers labels : app : customers spec : selector : app : customers ports : - port : 80 name : http targetPort : 3000 Each file defines its corresponding deployment, service account, and ClusterIP service. Apply the two files to your Kubernetes cluster. kubectl apply -f customers.yaml kubectl apply -f web-frontend.yaml Confirm that: Two pods are running, one for each service Each pod consists of two containers, the one running the service image, plus the Envoy sidecar kubectl get pod How did each pod end up with two containers? Istio installs a Kubernetes object known as a mutating webhook admission controller : logic that intercepts Kubernetes object creation requests and that has the permission to alter (mutate) what ends up stored in etcd (the pod spec). You can list the mutating webhooks in your Kubernetes cluster and confirm that the sidecar injector is present. kubectl get mutatingwebhookconfigurations Verify access to each service \u00b6 We wish to deploy a pod that runs a curl image so we can verify that each service is reachable from within the cluster. The Istio distribution provides a sample app called sleep that will serve this purpose. Deploy sleep to the default namespace. sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Use the kubectl exec command to call the customers service from the sleep pod. kubectl exec deploy/sleep -- curl -s customers The console output should show a list of customers in JSON format. Call the web-frontend service kubectl exec deploy/sleep -- curl -s web-frontend | head The console output should show the start of an HTML page listing customers in an HTML table. Next \u00b6 In the next lab, we expose the web-frontend using an Istio Ingress Gateway. This will allow us to access this application on the web.","title":"The application"},{"location":"the-app/#the-application","text":"In this lab you will deploy an application to your mesh. The application consists of two microservices, web-frontend and customers . Info The official Istio docs canonical example is the BookInfo application . For this workshop we felt that an application involving fewer microservices would be more clear. The customers service exposes a REST endpoint that returns a list of customers in JSON format. The web-frontend calls customers to retrieve the list, which it uses to render to HTML. The respective Docker images for these services have already been built and pushed to a Docker registry. You will deploy the application to the default Kubernetes namespace. But before proceeding, we must enable sidecar injection.","title":"The application"},{"location":"the-app/#enable-automatic-sidecar-injection","text":"There are two options for sidecar injection : automatic and manual. In this lab we will use automatic injection, which involves labeling the namespace where the pods are to reside. Label the default namespace kubectl label namespace default istio-injection = enabled Verify that the label has been applied: kubectl get ns -Listio-injection","title":"Enable automatic sidecar injection"},{"location":"the-app/#deploy-the-application","text":"Study the two Kubernetes yaml files: web-frontend.yaml and customers.yaml . web-frontend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : web-frontend --- apiVersion : apps/v1 kind : Deployment metadata : name : web-frontend labels : app : web-frontend spec : replicas : 1 selector : matchLabels : app : web-frontend template : metadata : labels : app : web-frontend version : v1 spec : serviceAccountName : web-frontend containers : - image : gcr.io/tetratelabs/web-frontend:1.0.0 imagePullPolicy : Always name : web ports : - containerPort : 8080 env : - name : CUSTOMER_SERVICE_URL value : \"http://customers.default.svc.cluster.local\" --- kind : Service apiVersion : v1 metadata : name : web-frontend labels : app : web-frontend spec : selector : app : web-frontend ports : - port : 80 name : http targetPort : 8080 customers.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- apiVersion : v1 kind : ServiceAccount metadata : name : customers --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v1 labels : app : customers version : v1 spec : replicas : 1 selector : matchLabels : app : customers version : v1 template : metadata : labels : app : customers version : v1 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : customers labels : app : customers spec : selector : app : customers ports : - port : 80 name : http targetPort : 3000 Each file defines its corresponding deployment, service account, and ClusterIP service. Apply the two files to your Kubernetes cluster. kubectl apply -f customers.yaml kubectl apply -f web-frontend.yaml Confirm that: Two pods are running, one for each service Each pod consists of two containers, the one running the service image, plus the Envoy sidecar kubectl get pod How did each pod end up with two containers? Istio installs a Kubernetes object known as a mutating webhook admission controller : logic that intercepts Kubernetes object creation requests and that has the permission to alter (mutate) what ends up stored in etcd (the pod spec). You can list the mutating webhooks in your Kubernetes cluster and confirm that the sidecar injector is present. kubectl get mutatingwebhookconfigurations","title":"Deploy the application"},{"location":"the-app/#verify-access-to-each-service","text":"We wish to deploy a pod that runs a curl image so we can verify that each service is reachable from within the cluster. The Istio distribution provides a sample app called sleep that will serve this purpose. Deploy sleep to the default namespace. sleep.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion : v1 kind : ServiceAccount metadata : name : sleep --- apiVersion : v1 kind : Service metadata : name : sleep labels : app : sleep service : sleep spec : ports : - port : 80 name : http selector : app : sleep --- apiVersion : apps/v1 kind : Deployment metadata : name : sleep spec : replicas : 1 selector : matchLabels : app : sleep template : metadata : labels : app : sleep spec : terminationGracePeriodSeconds : 0 serviceAccountName : sleep containers : - name : sleep image : curlimages/curl command : [ \"/bin/sleep\" , \"3650d\" ] imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /etc/sleep/tls name : secret-volume volumes : - name : secret-volume secret : secretName : sleep-secret optional : true --- kubectl apply -f sleep.yaml Use the kubectl exec command to call the customers service from the sleep pod. kubectl exec deploy/sleep -- curl -s customers The console output should show a list of customers in JSON format. Call the web-frontend service kubectl exec deploy/sleep -- curl -s web-frontend | head The console output should show the start of an HTML page listing customers in an HTML table.","title":"Verify access to each service"},{"location":"the-app/#next","text":"In the next lab, we expose the web-frontend using an Istio Ingress Gateway. This will allow us to access this application on the web.","title":"Next"},{"location":"traffic-shifting/","text":"Traffic shifting \u00b6 Version 2 of the customers service has been developed, and it's time to deploy it to production. Whereas version 1 returned a list of customer names, version 2 also includes each customer's city. Deploying customers, v2 \u00b6 We wish to deploy the new service but aren't yet ready to direct traffic to it. It would be prudent to separate the task of deploying the new service from the task of directing traffic to it. Labels \u00b6 The customers service is labeled with app=customers . Verify this with: kubectl get pod -Lapp,version Note the selector on the customers service: kubectl get svc customers -o wide If we were to just deploy v2, the selector would match both versions. DestinationRules \u00b6 We can inform Istio that two distinct subsets of the customers service exist, and we can use the version label as the discriminator. customers-destinationrule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : customers spec : host : customers.default.svc.cluster.local subsets : - name : v1 labels : version : v1 - name : v2 labels : version : v2 Apply the above destination rule to the cluster. Verify that it's been applied. kubectl get destinationrule VirtualServices \u00b6 Armed with two distinct destinations, the VirtualService custom resource allows us to define a routing rule that sends all traffic to the v1 subset. customers-virtualservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - route : - destination : host : customers.default.svc.cluster.local subset : v1 Above, note how the route specifies subset v1. Apply the virtual service to the cluster. Verify that it's been applied. kubectl get virtualservice Finally deploy customers, v2 \u00b6 Apply the following Kubernetes deployment to the cluster. customers-v2.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v2 labels : app : customers version : v2 spec : replicas : 1 selector : matchLabels : app : customers version : v2 template : metadata : labels : app : customers version : v2 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:2.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 Check that traffic routes strictly to v1 \u00b6 Generate some traffic. siege --delay = 3 --concurrent = 3 --time = 20M http:// $GATEWAY_IP / Open a separate terminal and launch the Kiali dashboard istioctl dashboard kiali Take a look at the graph. Select the default namespace and the Versioned app graph . The graph should show all traffic going to v1. Route to customers, v2 \u00b6 We wish to proceed with caution. Before customers can see version 2, we want to make sure that the service functions properly. Expose \"debug\" traffic to v2 \u00b6 Review this proposed updated routing specification. customers-vs-debug.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - match : - headers : user-agent : exact : debug route : - destination : host : customers.default.svc.cluster.local subset : v2 - route : - destination : host : customers.default.svc.cluster.local subset : v1 We are telling Istio to check an HTTP header: if the user-agent is set to debug , route to v2, otherwise route to v1. Open a new terminal and apply the above resource to the cluster; it will overwrite the currently defined virtualservice as both yamls use the same resource name. kubectl apply -f customers-vs-debug.yaml Test it \u00b6 Open a browser and visit the application. If you need it GATEWAY_IP = $( kubectl get svc -n istio-system istio-ingressgateway -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) We can tell v1 and v2 apart in that v2 displays not only customer names but also their city (in two columns). If you're using Chrome or Firefox, you can customize the user-agent header as follows: Open the browser's developer tools Open the \"three dots\" menu, and select More tools \u2192 Network conditions The network conditions panel will open Under User agent , uncheck Use browser default Select Custom... and in the text field enter debug Refresh the page; traffic should be directed to v2. Tip If you refresh the page a good dozen times and then wait ~15-30 seconds, you should see some of that v2 traffic in Kiali. Canary \u00b6 Well, v2 looks good; we decide to expose the new version to the public, but we're still prudent. Start by siphoning 10% of traffic over to v2. customers-vs-canary.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - route : - destination : host : customers.default.svc.cluster.local subset : v2 weight : 10 - destination : host : customers.default.svc.cluster.local subset : v1 weight : 90 Above, note the weight field specifying 10 percent of traffic to v2. Kiali should now show traffic going to both v1 and v2. Apply the above resource. In your browser: undo the user agent customization and refresh the page a bunch of times. Most of the requests still go to v1, but some are directed to v2. Check Grafana \u00b6 Before we open the floodgates, we wish to determine how v2 is fairing. istioctl dashboard grafana In Grafana, visit the Istio Workload Dashboard and specifically look at the customers v2 workload. Look at the request rate and the incoming success rate, also the latencies. If all looks good, up the percentage from 90/10 to, say 50/50. Watch the request volume change (you may need to click on the \"refresh dashboard\" button in the upper right-hand corner). Finally, switch all traffic over to v2. customers-virtualservice-final.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - route : - destination : host : customers.default.svc.cluster.local subset : v2 After you apply the above yaml, go to your browser and make sure all requests land on v2 (2-column output). Within a minute or so, the Kiali dashboard should also reflect the fact that all traffic is going to the customers v2 service. Though it no longer receives any traffic, we decide to leave v1 running a while longer before retiring it. Cleanup \u00b6 After completing this lab, reset your application to its initial state: Delete the customers virtual service: kubectl delete virtualservice customers Delete the destination rule for the customers service: kubectl delete destinationrule customers Delete the customer-v2 deployment: kubectl delete deploy customers-v2","title":"Traffic shifting"},{"location":"traffic-shifting/#traffic-shifting","text":"Version 2 of the customers service has been developed, and it's time to deploy it to production. Whereas version 1 returned a list of customer names, version 2 also includes each customer's city.","title":"Traffic shifting"},{"location":"traffic-shifting/#deploying-customers-v2","text":"We wish to deploy the new service but aren't yet ready to direct traffic to it. It would be prudent to separate the task of deploying the new service from the task of directing traffic to it.","title":"Deploying customers, v2"},{"location":"traffic-shifting/#labels","text":"The customers service is labeled with app=customers . Verify this with: kubectl get pod -Lapp,version Note the selector on the customers service: kubectl get svc customers -o wide If we were to just deploy v2, the selector would match both versions.","title":"Labels"},{"location":"traffic-shifting/#destinationrules","text":"We can inform Istio that two distinct subsets of the customers service exist, and we can use the version label as the discriminator. customers-destinationrule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : customers spec : host : customers.default.svc.cluster.local subsets : - name : v1 labels : version : v1 - name : v2 labels : version : v2 Apply the above destination rule to the cluster. Verify that it's been applied. kubectl get destinationrule","title":"DestinationRules"},{"location":"traffic-shifting/#virtualservices","text":"Armed with two distinct destinations, the VirtualService custom resource allows us to define a routing rule that sends all traffic to the v1 subset. customers-virtualservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - route : - destination : host : customers.default.svc.cluster.local subset : v1 Above, note how the route specifies subset v1. Apply the virtual service to the cluster. Verify that it's been applied. kubectl get virtualservice","title":"VirtualServices"},{"location":"traffic-shifting/#finally-deploy-customers-v2","text":"Apply the following Kubernetes deployment to the cluster. customers-v2.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- apiVersion : apps/v1 kind : Deployment metadata : name : customers-v2 labels : app : customers version : v2 spec : replicas : 1 selector : matchLabels : app : customers version : v2 template : metadata : labels : app : customers version : v2 spec : serviceAccountName : customers containers : - image : gcr.io/tetratelabs/customers:2.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000","title":"Finally deploy customers, v2"},{"location":"traffic-shifting/#check-that-traffic-routes-strictly-to-v1","text":"Generate some traffic. siege --delay = 3 --concurrent = 3 --time = 20M http:// $GATEWAY_IP / Open a separate terminal and launch the Kiali dashboard istioctl dashboard kiali Take a look at the graph. Select the default namespace and the Versioned app graph . The graph should show all traffic going to v1.","title":"Check that traffic routes strictly to v1"},{"location":"traffic-shifting/#route-to-customers-v2","text":"We wish to proceed with caution. Before customers can see version 2, we want to make sure that the service functions properly.","title":"Route to customers, v2"},{"location":"traffic-shifting/#expose-debug-traffic-to-v2","text":"Review this proposed updated routing specification. customers-vs-debug.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - match : - headers : user-agent : exact : debug route : - destination : host : customers.default.svc.cluster.local subset : v2 - route : - destination : host : customers.default.svc.cluster.local subset : v1 We are telling Istio to check an HTTP header: if the user-agent is set to debug , route to v2, otherwise route to v1. Open a new terminal and apply the above resource to the cluster; it will overwrite the currently defined virtualservice as both yamls use the same resource name. kubectl apply -f customers-vs-debug.yaml","title":"Expose \"debug\" traffic to v2"},{"location":"traffic-shifting/#test-it","text":"Open a browser and visit the application. If you need it GATEWAY_IP = $( kubectl get svc -n istio-system istio-ingressgateway -ojsonpath = '{.status.loadBalancer.ingress[0].ip}' ) We can tell v1 and v2 apart in that v2 displays not only customer names but also their city (in two columns). If you're using Chrome or Firefox, you can customize the user-agent header as follows: Open the browser's developer tools Open the \"three dots\" menu, and select More tools \u2192 Network conditions The network conditions panel will open Under User agent , uncheck Use browser default Select Custom... and in the text field enter debug Refresh the page; traffic should be directed to v2. Tip If you refresh the page a good dozen times and then wait ~15-30 seconds, you should see some of that v2 traffic in Kiali.","title":"Test it"},{"location":"traffic-shifting/#canary","text":"Well, v2 looks good; we decide to expose the new version to the public, but we're still prudent. Start by siphoning 10% of traffic over to v2. customers-vs-canary.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - route : - destination : host : customers.default.svc.cluster.local subset : v2 weight : 10 - destination : host : customers.default.svc.cluster.local subset : v1 weight : 90 Above, note the weight field specifying 10 percent of traffic to v2. Kiali should now show traffic going to both v1 and v2. Apply the above resource. In your browser: undo the user agent customization and refresh the page a bunch of times. Most of the requests still go to v1, but some are directed to v2.","title":"Canary"},{"location":"traffic-shifting/#check-grafana","text":"Before we open the floodgates, we wish to determine how v2 is fairing. istioctl dashboard grafana In Grafana, visit the Istio Workload Dashboard and specifically look at the customers v2 workload. Look at the request rate and the incoming success rate, also the latencies. If all looks good, up the percentage from 90/10 to, say 50/50. Watch the request volume change (you may need to click on the \"refresh dashboard\" button in the upper right-hand corner). Finally, switch all traffic over to v2. customers-virtualservice-final.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : customers spec : hosts : - customers.default.svc.cluster.local http : - route : - destination : host : customers.default.svc.cluster.local subset : v2 After you apply the above yaml, go to your browser and make sure all requests land on v2 (2-column output). Within a minute or so, the Kiali dashboard should also reflect the fact that all traffic is going to the customers v2 service. Though it no longer receives any traffic, we decide to leave v1 running a while longer before retiring it.","title":"Check Grafana"},{"location":"traffic-shifting/#cleanup","text":"After completing this lab, reset your application to its initial state: Delete the customers virtual service: kubectl delete virtualservice customers Delete the destination rule for the customers service: kubectl delete destinationrule customers Delete the customer-v2 deployment: kubectl delete deploy customers-v2","title":"Cleanup"},{"location":"vm-workloads/","text":"VM workloads \u00b6 Istio supports running workloads outside of Kubernetes. The canonical example is a legacy application that, for some reason, is not containerized and must run on a virtual machine (VM). To explore onboarding VM workloads with Istio , let us pretend that our venerable customers service is such a legacy application. In this lab, you will: Enable [VM] workload entry auto-registration (a feature considered experimental at time of writing) Provision a VM and deploy the customers app on it. Enable data-plane communication between the VM workload and the mesh Configure control-plane communication between istiod and the VM Define a WorkloadGroup, a concept analogous to Kubernetes Deployment resource Install and configure the Envoy sidecar on the VM The fruit of this labor will be to witness the web-frontend service successfully make calls to the customers backend running on the VM. The instructions in this lab are specific to Google Cloud Platform (GCP). Set default compute region and zone \u00b6 Configuring the default region and zone helps avoid specifying them in subsequent commands: gcloud config set compute/region us-west1 gcloud config set compute/zone us-west1-a Enable WorkloadEntry auto-registration \u00b6 Update the mesh configuration to turn on this capability: istioctl install \\ --set values.pilot.env.PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION = true Provision the VM \u00b6 gcloud compute instances create my-mesh-vm \\ --tags = mesh-vm \\ --machine-type = n1-standard-2 Above, we tag the VM to simplify configuring communication to the VM. Deploy customers to the VM \u00b6 Ssh to the VM: gcloud compute ssh ubuntu@my-mesh-vm The most expedient way to deploy the customers app is to leverage its Docker image. So.. Install docker: sudo apt-get update sudo apt-get -y install docker.io Run the container, using version 2.0 to easily distinguish this workload from the one already running on Kubernetes: sudo docker run -d --name customers \\ --net host \\ gcr.io/tetratelabs/customers:2.0.0 Verify that the customers app is running: curl localhost:3000 Enable data-plane communication \u00b6 In this scenario, both the VM and the cluster exist in the same network. Use a firewall rule to enable communication from the Pod IP addresses to the VM in question. Capture the Pod IP address range: export CLUSTER_POD_CIDR = $( gcloud container clusters list --format = json | jq -r '.[0].clusterIpv4Cidr' ) Apply the firewall rule: gcloud compute firewall-rules create \"cluster-pods-to-vm\" \\ --source-ranges = $CLUSTER_POD_CIDR \\ --target-tags = mesh-vm \\ --action = allow \\ --rules = tcp:3000 Info The path of communication between the VM workload and the Kubernetes cluster depends on the network topology. In scenarios where the VM and the cluster are on separate networks, that communication path is altered to use an \"east-west\" ingress gateway. Configure control-plane communication \u00b6 Deploy an additional Istio Ingress Gateway, that we label as \"eastwest\". The Istio distribution provides a simple script for this purpose. First, navigate to your Istio distribution folder, then run: ./samples/multicluster/gen-eastwest-gateway.sh --single-cluster | istioctl install -y -f - Confirm the presence of the new \"eastwest\" gateway deployment: kubectl get pod -n istio-system Expose the services of istiod with a Gateway and VirtualService resource. Here too, the Istio distribution provides the recipe: kubectl apply -n istio-system -f ./samples/multicluster/expose-istiod.yaml Info The above resources (Gateway and VirtualService) configure the eastwest-gateway to listen on ports 15012 (used for control plane - istiod) and 15017 (used for pilot discovery validation webhook). The VirtualService matches the requests on the two ports and then routes them to istiod to appropriate ports. Define a WorkloadGroup \u00b6 Istio provides the WorkloadGroup and WorkloadEntry resources; they are the VM analogs of the Kubernetes Deployment and Pod resources. customers-workloadgroup.yaml 1 2 3 4 5 6 7 8 9 10 11 12 --- apiVersion : networking.istio.io/v1alpha3 kind : WorkloadGroup metadata : name : customers namespace : default spec : metadata : labels : app : customers template : serviceAccount : customers This resource provides a template for Workload Entries, and ensures that newly-created workload entries are associated with the existing customers service account, for workload identity purposes. Apply the above to the cluster: kubectl apply -f customers-workloadgroup.yaml Install and configure the Envoy sidecar on the VM \u00b6 Generate the configuration files \u00b6 The istioctl CLI provides a command to automate the generation of all files needed to configure the Envoy sidecar on the VM: Create a folder to contain these files: mkdir vm_files Run the command to generate the VM files: istioctl x workload entry configure \\ --file customers-workloadgroup.yaml \\ --output vm_files \\ --autoregister VM onboarding is not a one-time action Whenever we update the WorkloadGroup resource, we also have to re-create the configuration (run the istioctl x workload entry configure command) and copy the generated files to the VM. Inspect the contents of the vm_files folder. Here is how each file is described in the Istio documentation : cluster.env : Contains metadata that identifies what namespace, service account, network CIDR and (optionally) what inbound ports to capture. istio-token : A Kubernetes token used to get certs from the CA. mesh.yaml : Provides ProxyConfig to configure discoveryAddress , health-checking probes, and some authentication options. root-cert.pem : The root certificate used to authenticate. hosts : An addendum to /etc/hosts that the proxy will use to reach istiod for xDS. Info To onboard the VM workload the sidecar also nees the Istio mTLS certificate. When the sidcear starts it uses the istio-token file to prove its identity to Istio control plane and to obtain the Istio mTLS certificate. The mTLS certificate is stored on disk for cases when the VM restarts. The expiration time on the initial istio-token is 1 hour. If it takes you longer than 1 hour to onboard the VM, you' ll need to re-run the istioctl x workload entry configure command to generate a new token. Once the VM obtains the mTLS certificate, it can use it to prove its identity. The certificate is valid for 24 hours and is automatically rotated before it expires. Transfer the files to the VM \u00b6 Use the following command to copy the contents of the vm_files folder to the VM: gcloud compute scp vm_files/* ubuntu@my-mesh-vm: Configure the VM \u00b6 SSH onto the VM gcloud compute ssh my-mesh-vm Install the root certificate to /etc/certs : sudo mkdir -p /etc/certs sudo cp ~/root-cert.pem /etc/certs/root-cert.pem Install the token to /var/run/secrets/tokens : sudo mkdir -p /var/run/secrets/tokens sudo cp ~/istio-token /var/run/secrets/tokens/istio-token Download and install the Istio sidecar package: curl -LO https://storage.googleapis.com/istio-release/releases/1.18.2/deb/istio-sidecar.deb sudo dpkg -i istio-sidecar.deb Install cluster.env to /var/lib/istio/envoy/ : sudo cp ~/cluster.env /var/lib/istio/envoy/cluster.env Install the Mesh Config to /etc/istio/config/mesh : sudo cp ~/mesh.yaml /etc/istio/config/mesh Add the istiod host to /etc/hosts : sudo sh -c 'cat $(eval echo ~$SUDO_USER)/hosts >> /etc/hosts' Change the ownership of files in /etc/certs and /var/lib/istio/envoy to the Istio proxy: sudo mkdir -p /etc/istio/proxy sudo chown -R istio-proxy /etc/certs /var/run/secrets /var/lib/istio /etc/istio/config /etc/istio/proxy With all files in place, we are ready to start the sidecar on the VM. Start the sidecar \u00b6 Because we used the VM auto-registration, Istio automatically creates the WorkloadEntry resource for us shortly after we start the Istio service on the VM. Watch for Workloadentry resources using the --watch flag kubectl get workloadentry --watch On the VM, start the Istio service: sudo systemctl start istio See the WorkloadEntry appear: NAME AGE ADDRESS customers-10.128.0.7 12s 10.128.0.7 Press Ctrl-C to stop watching for changes. On the VM, check that the istio service is running: sudo systemctl status istio.service The full logs can be accessed with journalctl (or by tailing /var/log/istio/istio.log ): sudo journalctl -u istio.service Test it! \u00b6 Confirm that the customer workload is accessible from the Kubernetes cluster. Confirm that web-frontend now has two endpoints for the customers service: istioctl proxy-config endpoints deploy/web-frontend.default | grep customers Open a browser to your $GATEWAY_IP and see whether you can get the two-column version of the customers service to display (you may need to refresh the page once or twice) Call customers directly from sleep : kubectl exec deploy/sleep -- curl -s customers Access services from the VM \u00b6 Try to access the helloworld service running on Kubernetes from the VM : curl http://helloworld.default:5000/hello","title":"VM workloads"},{"location":"vm-workloads/#vm-workloads","text":"Istio supports running workloads outside of Kubernetes. The canonical example is a legacy application that, for some reason, is not containerized and must run on a virtual machine (VM). To explore onboarding VM workloads with Istio , let us pretend that our venerable customers service is such a legacy application. In this lab, you will: Enable [VM] workload entry auto-registration (a feature considered experimental at time of writing) Provision a VM and deploy the customers app on it. Enable data-plane communication between the VM workload and the mesh Configure control-plane communication between istiod and the VM Define a WorkloadGroup, a concept analogous to Kubernetes Deployment resource Install and configure the Envoy sidecar on the VM The fruit of this labor will be to witness the web-frontend service successfully make calls to the customers backend running on the VM. The instructions in this lab are specific to Google Cloud Platform (GCP).","title":"VM workloads"},{"location":"vm-workloads/#set-default-compute-region-and-zone","text":"Configuring the default region and zone helps avoid specifying them in subsequent commands: gcloud config set compute/region us-west1 gcloud config set compute/zone us-west1-a","title":"Set default compute region and zone"},{"location":"vm-workloads/#enable-workloadentry-auto-registration","text":"Update the mesh configuration to turn on this capability: istioctl install \\ --set values.pilot.env.PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION = true","title":"Enable WorkloadEntry auto-registration"},{"location":"vm-workloads/#provision-the-vm","text":"gcloud compute instances create my-mesh-vm \\ --tags = mesh-vm \\ --machine-type = n1-standard-2 Above, we tag the VM to simplify configuring communication to the VM.","title":"Provision the VM"},{"location":"vm-workloads/#deploy-customers-to-the-vm","text":"Ssh to the VM: gcloud compute ssh ubuntu@my-mesh-vm The most expedient way to deploy the customers app is to leverage its Docker image. So.. Install docker: sudo apt-get update sudo apt-get -y install docker.io Run the container, using version 2.0 to easily distinguish this workload from the one already running on Kubernetes: sudo docker run -d --name customers \\ --net host \\ gcr.io/tetratelabs/customers:2.0.0 Verify that the customers app is running: curl localhost:3000","title":"Deploy customers to the VM"},{"location":"vm-workloads/#enable-data-plane-communication","text":"In this scenario, both the VM and the cluster exist in the same network. Use a firewall rule to enable communication from the Pod IP addresses to the VM in question. Capture the Pod IP address range: export CLUSTER_POD_CIDR = $( gcloud container clusters list --format = json | jq -r '.[0].clusterIpv4Cidr' ) Apply the firewall rule: gcloud compute firewall-rules create \"cluster-pods-to-vm\" \\ --source-ranges = $CLUSTER_POD_CIDR \\ --target-tags = mesh-vm \\ --action = allow \\ --rules = tcp:3000 Info The path of communication between the VM workload and the Kubernetes cluster depends on the network topology. In scenarios where the VM and the cluster are on separate networks, that communication path is altered to use an \"east-west\" ingress gateway.","title":"Enable data-plane communication"},{"location":"vm-workloads/#configure-control-plane-communication","text":"Deploy an additional Istio Ingress Gateway, that we label as \"eastwest\". The Istio distribution provides a simple script for this purpose. First, navigate to your Istio distribution folder, then run: ./samples/multicluster/gen-eastwest-gateway.sh --single-cluster | istioctl install -y -f - Confirm the presence of the new \"eastwest\" gateway deployment: kubectl get pod -n istio-system Expose the services of istiod with a Gateway and VirtualService resource. Here too, the Istio distribution provides the recipe: kubectl apply -n istio-system -f ./samples/multicluster/expose-istiod.yaml Info The above resources (Gateway and VirtualService) configure the eastwest-gateway to listen on ports 15012 (used for control plane - istiod) and 15017 (used for pilot discovery validation webhook). The VirtualService matches the requests on the two ports and then routes them to istiod to appropriate ports.","title":"Configure control-plane communication"},{"location":"vm-workloads/#define-a-workloadgroup","text":"Istio provides the WorkloadGroup and WorkloadEntry resources; they are the VM analogs of the Kubernetes Deployment and Pod resources. customers-workloadgroup.yaml 1 2 3 4 5 6 7 8 9 10 11 12 --- apiVersion : networking.istio.io/v1alpha3 kind : WorkloadGroup metadata : name : customers namespace : default spec : metadata : labels : app : customers template : serviceAccount : customers This resource provides a template for Workload Entries, and ensures that newly-created workload entries are associated with the existing customers service account, for workload identity purposes. Apply the above to the cluster: kubectl apply -f customers-workloadgroup.yaml","title":"Define a WorkloadGroup"},{"location":"vm-workloads/#install-and-configure-the-envoy-sidecar-on-the-vm","text":"","title":"Install and configure the Envoy sidecar on the VM"},{"location":"vm-workloads/#generate-the-configuration-files","text":"The istioctl CLI provides a command to automate the generation of all files needed to configure the Envoy sidecar on the VM: Create a folder to contain these files: mkdir vm_files Run the command to generate the VM files: istioctl x workload entry configure \\ --file customers-workloadgroup.yaml \\ --output vm_files \\ --autoregister VM onboarding is not a one-time action Whenever we update the WorkloadGroup resource, we also have to re-create the configuration (run the istioctl x workload entry configure command) and copy the generated files to the VM. Inspect the contents of the vm_files folder. Here is how each file is described in the Istio documentation : cluster.env : Contains metadata that identifies what namespace, service account, network CIDR and (optionally) what inbound ports to capture. istio-token : A Kubernetes token used to get certs from the CA. mesh.yaml : Provides ProxyConfig to configure discoveryAddress , health-checking probes, and some authentication options. root-cert.pem : The root certificate used to authenticate. hosts : An addendum to /etc/hosts that the proxy will use to reach istiod for xDS. Info To onboard the VM workload the sidecar also nees the Istio mTLS certificate. When the sidcear starts it uses the istio-token file to prove its identity to Istio control plane and to obtain the Istio mTLS certificate. The mTLS certificate is stored on disk for cases when the VM restarts. The expiration time on the initial istio-token is 1 hour. If it takes you longer than 1 hour to onboard the VM, you' ll need to re-run the istioctl x workload entry configure command to generate a new token. Once the VM obtains the mTLS certificate, it can use it to prove its identity. The certificate is valid for 24 hours and is automatically rotated before it expires.","title":"Generate the configuration files"},{"location":"vm-workloads/#transfer-the-files-to-the-vm","text":"Use the following command to copy the contents of the vm_files folder to the VM: gcloud compute scp vm_files/* ubuntu@my-mesh-vm:","title":"Transfer the files to the VM"},{"location":"vm-workloads/#configure-the-vm","text":"SSH onto the VM gcloud compute ssh my-mesh-vm Install the root certificate to /etc/certs : sudo mkdir -p /etc/certs sudo cp ~/root-cert.pem /etc/certs/root-cert.pem Install the token to /var/run/secrets/tokens : sudo mkdir -p /var/run/secrets/tokens sudo cp ~/istio-token /var/run/secrets/tokens/istio-token Download and install the Istio sidecar package: curl -LO https://storage.googleapis.com/istio-release/releases/1.18.2/deb/istio-sidecar.deb sudo dpkg -i istio-sidecar.deb Install cluster.env to /var/lib/istio/envoy/ : sudo cp ~/cluster.env /var/lib/istio/envoy/cluster.env Install the Mesh Config to /etc/istio/config/mesh : sudo cp ~/mesh.yaml /etc/istio/config/mesh Add the istiod host to /etc/hosts : sudo sh -c 'cat $(eval echo ~$SUDO_USER)/hosts >> /etc/hosts' Change the ownership of files in /etc/certs and /var/lib/istio/envoy to the Istio proxy: sudo mkdir -p /etc/istio/proxy sudo chown -R istio-proxy /etc/certs /var/run/secrets /var/lib/istio /etc/istio/config /etc/istio/proxy With all files in place, we are ready to start the sidecar on the VM.","title":"Configure the VM"},{"location":"vm-workloads/#start-the-sidecar","text":"Because we used the VM auto-registration, Istio automatically creates the WorkloadEntry resource for us shortly after we start the Istio service on the VM. Watch for Workloadentry resources using the --watch flag kubectl get workloadentry --watch On the VM, start the Istio service: sudo systemctl start istio See the WorkloadEntry appear: NAME AGE ADDRESS customers-10.128.0.7 12s 10.128.0.7 Press Ctrl-C to stop watching for changes. On the VM, check that the istio service is running: sudo systemctl status istio.service The full logs can be accessed with journalctl (or by tailing /var/log/istio/istio.log ): sudo journalctl -u istio.service","title":"Start the sidecar"},{"location":"vm-workloads/#test-it","text":"Confirm that the customer workload is accessible from the Kubernetes cluster. Confirm that web-frontend now has two endpoints for the customers service: istioctl proxy-config endpoints deploy/web-frontend.default | grep customers Open a browser to your $GATEWAY_IP and see whether you can get the two-column version of the customers service to display (you may need to refresh the page once or twice) Call customers directly from sleep : kubectl exec deploy/sleep -- curl -s customers","title":"Test it!"},{"location":"vm-workloads/#access-services-from-the-vm","text":"Try to access the helloworld service running on Kubernetes from the VM : curl http://helloworld.default:5000/hello","title":"Access services from the VM"},{"location":"vm_workloads/","text":"VM workloads \u00b6 In this lab, we will learn how to connect a workload running on a virtual machine to the Istio service mesh running on a Kubernetes cluster. Both Kubernetes cluster and the virtual machine will be running on Google Cloud Platform (GCP). Istio mesh installation \u00b6 After we've created a Kubernetes cluster, we can download, install Istio, and configure Istio. We are using a pre-alpha feature that automatically creates WorkloadEntries for our VMs. To support this, we have to set the PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION variable to true when installing Istio on the Kubernetes cluster. Since we already installed Istio, we'll just update the existing installation using the IstioOperator. One of the differences between regular Istio installation and one that supports VM workloads is in setting cluster name and network. In this lab we'll set the network name to an empty string because we'll only use a single network - i.e. both the VM and the Kubernetes will be part of the same network. If had a scenario where the VM was running in a different network as the Kubernetes cluster, we'd set the network name to differentiate the networks. Click for istio-vm-install.yaml istio-vm-install.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : profile : default values : global : meshID : mesh1 multiCluster : clusterName : Kubernetes network : \"\" pilot : env : PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION : true PILOT_ENABLE_WORKLOAD_ENTRY_HEALTHCHECKS : true Save the above YAML to istio-vm-install.yaml . We'll use istioctl to install Istio on the cluster: istioctl install -f istio-vm-install.yaml Once the installation completes, we can deploy a separate ingress gateway that will be used to expose the Istio's control plane to the virtual machine. Install and configure the east-west gateway \u00b6 The Istio package we downloaded contains a script we can use to generate the YAML that will deploy an Istio operator that creates the new gateway called istio-eastwestgateway . Go to the folder where you downloaded Istio (e.g. istio-1.18.2 ) and run this script: samples/multicluster/gen-eastwest-gateway.sh --single-cluster | istioctl install -y -f - You can list the Pods in the istio-system namespaces to check the gateway that was installed: kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-eastwestgateway-6d47fc4ddf-m89wn 1/1 Running 0 17s istio-egressgateway-7767c67b7c-wmpcw 1/1 Running 0 11m istio-ingressgateway-f5b854689-j9tcr 1/1 Running 0 11m istiod-65cc74b9c7-j5n4d 1/1 Running 0 4m59s For virtual machines to access the Istio's control plane, we need to create a Gateway resource to configure the istio-eastwestgateway , and a VirtualService that has the Gateway attached. We can use another script from the Istio package to create these resources and expose the control plane: kubectl apply -n istio-system -f samples/multicluster/expose-istiod.yaml gateway.networking.istio.io/istiod-gateway created virtualservice.networking.istio.io/istiod-vs created The above resources (Gateway and VirtualService) configure the eastwest-gateway to listen on ports 15012 (used for control plane - istiod) and 15017 (used for pilot discovery validation webhook). The VirtualService matches the requests on the two ports and then routes them to istiod to appropriate ports. The purpose of the eastwest-gateway in this scenario will be to allow the Istio sidecar proxy on the virtual machine to call the control plane running inside the cluster. The second role of the eastwest-gateway is in the scenarios where the VM and Kubernetes cluster are running in different networks. In that scenario, the workloads communicate to each through the eastwest-gateway. Prepare virtual machine namespace and files \u00b6 For the virtual machine workloads, we have to create a separate namespace to store the WorkloadEntry resource and any other VM workload related resources. Additionally, we will have to export the cluster environment file, token, certificate, and other files we will have to transfer to the virtual machine. Let's create a separate folder called vm-files to store these files. We can also save the full path to the folder in the WORK_DIR environment variable: mkdir -p vm-files export WORK_DIR = \" $PWD /vm-files\" We also set a couple of environment variables before continuing, so we don't have to re-type the values each time: export VM_APP = \"hello-vm\" export VM_NAMESPACE = \"vm-namespace\" export SERVICE_ACCOUNT = \"vm-sa\" Let's create the VM namespace and the service account we will use for VM workloads in the same namespace: kubectl create ns \" ${ VM_NAMESPACE } \" namespace/vm-namespace created kubectl create serviceaccount \" ${ SERVICE_ACCOUNT } \" -n \" ${ VM_NAMESPACE } \" serviceaccount/vm-sa created Next we'll create the WorkloadGroup resource using Istio CLI. Run the command to save the WorkloadGroup YAML to workloadgroup.yaml istioctl x workload group create --name \" ${ VM_APP } \" \\ --namespace \" ${ VM_NAMESPACE } \" \\ --labels app = \" ${ VM_APP } \" \\ --serviceAccount \" ${ SERVICE_ACCOUNT } \" > workloadgroup.yaml Here's how the contents of the workloadgroup.yaml should look like: apiVersion : networking.istio.io/v1alpha3 kind : WorkloadGroup metadata : name : hello-vm namespace : vm-namespace spec : metadata : labels : app : hello-vm template : serviceAccount : vm-sa Save the above YAML to workloadgroup.yaml and create the resource: kubectl apply -f workloadgroup.yaml Virtual machine needs information about the cluster and Istio's control plane to connect to it. To generate the required files, we can run istioctl x workload entry command. We save all generated files to the WORK_DIR : istioctl x workload entry configure \\ -f workloadgroup.yaml \\ -o \" ${ WORK_DIR } \" \\ --autoregister \\ --clusterID \"Kubernetes\" warning: a security token for namespace vm-namespace and service account vm-sa has been generated and stored at /vm-files/istio-token configuration generation into directory /vm-files was successful VM onboarding is not a one-time action Whenever we update the WorkloadGroup resource, we also have to re-create the configuration (run the istioctl x workload entry configure command) and copy the generated files to the virtual machine. The above command generates the following files: cluster.env : Contains metadata that identifies what namespace, service account, network CIDR and (optionally) what inbound ports to capture. istio-token : A Kubernetes token used to get certs from the CA. This is the the token from the vm-sa service account that's created using the istio-ca audience. mesh.yaml : Provides ProxyConfig to configure discoveryAddress , health-checking probes, and some authentication options. root-cert.pem : The root certificate used to authenticate. Comes from the istio-ca-root-cert ConfigMap in the vm-namespace namespace. hosts : An addendum to /etc/hosts that the proxy will use to reach istiod for xDS. We'll copy the these files to the virtual machine, to locations known by the Envoy proxy. Note that the location where the files are copied can be customized, but it requires updates to the initial Envoy proxy configuration. The files provide the Envoy proxy bootstrap configuration - they tell the proxy the information about the workload (e.g. the workload and service name and the namespace, the WorkloadGroup name, cluster ID and mesh ID and others). To onboard the VM workload the sidecar also nees the Istio mTLS certificate. When the sidcear starts it uses the istio-token file to prove its identity to Istio control plane and to obtain the Istio mTLS certificate. The mTLS certificate is stored on disk for cases when the VM restarts. The expiration time on the initial istio-token is 1 hour. If it takes you longer than 1 hour to onboard the VM, you' ll need to re-run the istioctl x workload entry configure command to generate a new token. Once the VM obtains the mTLS certificate, it can use it to prove its identity. The certificate is valid for 24 hours and the proxy will automatically refresh it before it expires. Set default compute region and zone \u00b6 Configuring the default and region and zone helps avoid specifying them in subsequent commands: gcloud config set compute/region us-west1 gcloud config set compute/zone us-west1-a Create the Virtual Machine \u00b6 We'll be running the virtual machine in GCP, just like the Kubernetes cluster. Create the VM gcloud compute instances create my-mesh-vm \\ --tags = mesh-vm \\ --machine-type = n1-standard-2 Obtain the cluster's Pod IP address range. export CLUSTER_POD_CIDR = $( gcloud container clusters list --format = json | jq -r '.[0].clusterIpv4Cidr' ) Create a firewall rule to allow ingress on port 80 from the cluster pods to the VM. gcloud compute firewall-rules create \"cluster-pods-to-vm\" \\ --source-ranges = $CLUSTER_POD_CIDR \\ --target-tags = mesh-vm \\ --action = allow \\ --rules = tcp:80 Apply configuration to the Virtual Machine \u00b6 Now it's time to configure the Virtual machine. In this example, we run a simple Python HTTP server on port 80. You could configure any other service on a different port. Just make sure you configure the security and firewall rules accordingly. Copy the files from vm-files folder to the home folder on the instance. gcloud compute scp vm-files/* my-mesh-vm:~ cluster.env 100% 627 626.9KB/s 00:00 hosts 100% 36 50.7KB/s 00:00 istio-token 100% 905 1.4MB/s 00:00 mesh.yaml 100% 668 918.7KB/s 00:00 root-cert.pem 100% 1094 1.7MB/s 00:00 SSH into the instance and copy the root certificate to /etc/certs (you can use the command from the instance details page in the SSH dropdown): gcloud compute ssh my-mesh-vm sudo mkdir -p /etc/certs sudo cp root-cert.pem /etc/certs/root-cert.pem Copy the istio-token file to /var/run/secrets/tokens folder: sudo mkdir -p /var/run/secrets/tokens sudo cp istio-token /var/run/secrets/tokens/istio-token Download and install the Istio sidecar package: curl -LO https://storage.googleapis.com/istio-release/releases/1.18.2/deb/istio-sidecar.deb sudo dpkg -i istio-sidecar.deb Copy cluster.env to /var/lib/istio/envoy/ : sudo cp cluster.env /var/lib/istio/envoy/cluster.env Copy Mesh config ( mesh.yaml ) to /etc/istio/config/mesh : sudo cp mesh.yaml /etc/istio/config/mesh Add the istiod host to the /etc/hosts file: sudo sh -c 'cat $(eval echo ~$SUDO_USER)/hosts >> /etc/hosts' Change the ownership of files in /etc/certs and /var/lib/istio/envoy to the Istio proxy: sudo mkdir -p /etc/istio/proxy sudo chown -R istio-proxy /var/lib/istio /etc/certs /etc/istio/proxy /etc/istio/config /var/run/secrets /etc/certs/root-cert.pem With all files in place, we are ready to start Istio on the virtual machine. Because we used the VM auto-registration, Istio automatically creates the WorkloadEntry resource for us shortly after we start the Istio service on the VM. You can see the WorkloadEntry creation in action as follows: Watch for Workloadentry resources using the --watch flag kubectl get workloadentry -n vm-namespace --watch On the VM, start the Istio service: sudo systemctl start istio You should see the WorkloadEntry appear NAME AGE ADDRESS hello-vm-10.128.0.7 12m 10.128.0.7 Press Ctrl-C to stop watching for changes. On the VM, you can check that the istio service is running with systemctl status istio . Alternatively, we can look at the contents of the /var/log/istio/istio.log to see that the proxy was successfully started. At this point, the virtual machine is configured to talk with the Istio's control plane in the Kubernetes cluster. Access services from the virtual machine \u00b6 From a different terminal window, we can now deploy a Hello world application to the Kubernetes cluster Click for hello-world.yaml hello-world.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : apps/v1 kind : Deployment metadata : name : hello-world labels : app : hello-world spec : replicas : 1 selector : matchLabels : app : hello-world template : metadata : labels : app : hello-world spec : containers : - image : gcr.io/tetratelabs/hello-world:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : hello-world labels : app : hello-world spec : selector : app : hello-world ports : - port : 80 name : http targetPort : 3000 Save the above YAML to hello-world.yaml and deploy it: kubectl apply -f hello-world.yaml Wait for the Pods to become ready and then go back to the virtual machine and try to access the Kubernetes service: curl http://hello-world.default Hello World You can access any service running within your Kubernetes cluster from the virtual machine. Run services on the virtual machine \u00b6 We can also run a workload on the virtual machine. Switch to the virtual machine instance and run a simple Python HTTP server: sudo python3 -m http.server 80 Serving HTTP on 0.0.0.0 port 80 (http://0.0.0.0:80/) ... What we want to do next is add the workload (Python HTTP service) to the mesh. For that reason, we created the VM namespace earlier. So let's create a Kubernetes service that represents the VM workload. Note that the name and the label values equal to the value of the VM_APP environment variable we set earlier. Don't forget to deploy the service to the VM_NAMESPACE . hello-vm-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Service metadata : name : hello-vm namespace : vm-namespace labels : app : hello-vm spec : ports : - port : 80 name : http-vm targetPort : 80 selector : app : hello-vm Save the above YAML to hello-vm-service.yaml and deploy it to the VM namespace: kubectl apply -f hello-vm-service.yaml We can now use the Kubernetes service name hello-vm.vm-namespace to access the workload running on the virtual machine (the Python server). Let's run a Pod inside the cluster and try to access the service from there: kubectl run curl --image = radial/busyboxplus:curl -i --tty --rm If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ After you get the command prompt in the Pod, you can run curl and access the workload. You should see a directory listing response. Similarly, you will notice a log entry on the instance where the HTTP server is running: [ root@curl:/ ] $ curl hello-vm.vm-namespace <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dt d\"> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"> <title>Directory listing for /</title> </head> <body> <h1>Directory listing for /</h1> <hr> ...","title":"VM workloads"},{"location":"vm_workloads/#vm-workloads","text":"In this lab, we will learn how to connect a workload running on a virtual machine to the Istio service mesh running on a Kubernetes cluster. Both Kubernetes cluster and the virtual machine will be running on Google Cloud Platform (GCP).","title":"VM workloads"},{"location":"vm_workloads/#istio-mesh-installation","text":"After we've created a Kubernetes cluster, we can download, install Istio, and configure Istio. We are using a pre-alpha feature that automatically creates WorkloadEntries for our VMs. To support this, we have to set the PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION variable to true when installing Istio on the Kubernetes cluster. Since we already installed Istio, we'll just update the existing installation using the IstioOperator. One of the differences between regular Istio installation and one that supports VM workloads is in setting cluster name and network. In this lab we'll set the network name to an empty string because we'll only use a single network - i.e. both the VM and the Kubernetes will be part of the same network. If had a scenario where the VM was running in a different network as the Kubernetes cluster, we'd set the network name to differentiate the networks. Click for istio-vm-install.yaml istio-vm-install.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : profile : default values : global : meshID : mesh1 multiCluster : clusterName : Kubernetes network : \"\" pilot : env : PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION : true PILOT_ENABLE_WORKLOAD_ENTRY_HEALTHCHECKS : true Save the above YAML to istio-vm-install.yaml . We'll use istioctl to install Istio on the cluster: istioctl install -f istio-vm-install.yaml Once the installation completes, we can deploy a separate ingress gateway that will be used to expose the Istio's control plane to the virtual machine.","title":"Istio mesh installation"},{"location":"vm_workloads/#install-and-configure-the-east-west-gateway","text":"The Istio package we downloaded contains a script we can use to generate the YAML that will deploy an Istio operator that creates the new gateway called istio-eastwestgateway . Go to the folder where you downloaded Istio (e.g. istio-1.18.2 ) and run this script: samples/multicluster/gen-eastwest-gateway.sh --single-cluster | istioctl install -y -f - You can list the Pods in the istio-system namespaces to check the gateway that was installed: kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-eastwestgateway-6d47fc4ddf-m89wn 1/1 Running 0 17s istio-egressgateway-7767c67b7c-wmpcw 1/1 Running 0 11m istio-ingressgateway-f5b854689-j9tcr 1/1 Running 0 11m istiod-65cc74b9c7-j5n4d 1/1 Running 0 4m59s For virtual machines to access the Istio's control plane, we need to create a Gateway resource to configure the istio-eastwestgateway , and a VirtualService that has the Gateway attached. We can use another script from the Istio package to create these resources and expose the control plane: kubectl apply -n istio-system -f samples/multicluster/expose-istiod.yaml gateway.networking.istio.io/istiod-gateway created virtualservice.networking.istio.io/istiod-vs created The above resources (Gateway and VirtualService) configure the eastwest-gateway to listen on ports 15012 (used for control plane - istiod) and 15017 (used for pilot discovery validation webhook). The VirtualService matches the requests on the two ports and then routes them to istiod to appropriate ports. The purpose of the eastwest-gateway in this scenario will be to allow the Istio sidecar proxy on the virtual machine to call the control plane running inside the cluster. The second role of the eastwest-gateway is in the scenarios where the VM and Kubernetes cluster are running in different networks. In that scenario, the workloads communicate to each through the eastwest-gateway.","title":"Install and configure the east-west gateway"},{"location":"vm_workloads/#prepare-virtual-machine-namespace-and-files","text":"For the virtual machine workloads, we have to create a separate namespace to store the WorkloadEntry resource and any other VM workload related resources. Additionally, we will have to export the cluster environment file, token, certificate, and other files we will have to transfer to the virtual machine. Let's create a separate folder called vm-files to store these files. We can also save the full path to the folder in the WORK_DIR environment variable: mkdir -p vm-files export WORK_DIR = \" $PWD /vm-files\" We also set a couple of environment variables before continuing, so we don't have to re-type the values each time: export VM_APP = \"hello-vm\" export VM_NAMESPACE = \"vm-namespace\" export SERVICE_ACCOUNT = \"vm-sa\" Let's create the VM namespace and the service account we will use for VM workloads in the same namespace: kubectl create ns \" ${ VM_NAMESPACE } \" namespace/vm-namespace created kubectl create serviceaccount \" ${ SERVICE_ACCOUNT } \" -n \" ${ VM_NAMESPACE } \" serviceaccount/vm-sa created Next we'll create the WorkloadGroup resource using Istio CLI. Run the command to save the WorkloadGroup YAML to workloadgroup.yaml istioctl x workload group create --name \" ${ VM_APP } \" \\ --namespace \" ${ VM_NAMESPACE } \" \\ --labels app = \" ${ VM_APP } \" \\ --serviceAccount \" ${ SERVICE_ACCOUNT } \" > workloadgroup.yaml Here's how the contents of the workloadgroup.yaml should look like: apiVersion : networking.istio.io/v1alpha3 kind : WorkloadGroup metadata : name : hello-vm namespace : vm-namespace spec : metadata : labels : app : hello-vm template : serviceAccount : vm-sa Save the above YAML to workloadgroup.yaml and create the resource: kubectl apply -f workloadgroup.yaml Virtual machine needs information about the cluster and Istio's control plane to connect to it. To generate the required files, we can run istioctl x workload entry command. We save all generated files to the WORK_DIR : istioctl x workload entry configure \\ -f workloadgroup.yaml \\ -o \" ${ WORK_DIR } \" \\ --autoregister \\ --clusterID \"Kubernetes\" warning: a security token for namespace vm-namespace and service account vm-sa has been generated and stored at /vm-files/istio-token configuration generation into directory /vm-files was successful VM onboarding is not a one-time action Whenever we update the WorkloadGroup resource, we also have to re-create the configuration (run the istioctl x workload entry configure command) and copy the generated files to the virtual machine. The above command generates the following files: cluster.env : Contains metadata that identifies what namespace, service account, network CIDR and (optionally) what inbound ports to capture. istio-token : A Kubernetes token used to get certs from the CA. This is the the token from the vm-sa service account that's created using the istio-ca audience. mesh.yaml : Provides ProxyConfig to configure discoveryAddress , health-checking probes, and some authentication options. root-cert.pem : The root certificate used to authenticate. Comes from the istio-ca-root-cert ConfigMap in the vm-namespace namespace. hosts : An addendum to /etc/hosts that the proxy will use to reach istiod for xDS. We'll copy the these files to the virtual machine, to locations known by the Envoy proxy. Note that the location where the files are copied can be customized, but it requires updates to the initial Envoy proxy configuration. The files provide the Envoy proxy bootstrap configuration - they tell the proxy the information about the workload (e.g. the workload and service name and the namespace, the WorkloadGroup name, cluster ID and mesh ID and others). To onboard the VM workload the sidecar also nees the Istio mTLS certificate. When the sidcear starts it uses the istio-token file to prove its identity to Istio control plane and to obtain the Istio mTLS certificate. The mTLS certificate is stored on disk for cases when the VM restarts. The expiration time on the initial istio-token is 1 hour. If it takes you longer than 1 hour to onboard the VM, you' ll need to re-run the istioctl x workload entry configure command to generate a new token. Once the VM obtains the mTLS certificate, it can use it to prove its identity. The certificate is valid for 24 hours and the proxy will automatically refresh it before it expires.","title":"Prepare virtual machine namespace and files"},{"location":"vm_workloads/#set-default-compute-region-and-zone","text":"Configuring the default and region and zone helps avoid specifying them in subsequent commands: gcloud config set compute/region us-west1 gcloud config set compute/zone us-west1-a","title":"Set default compute region and zone"},{"location":"vm_workloads/#create-the-virtual-machine","text":"We'll be running the virtual machine in GCP, just like the Kubernetes cluster. Create the VM gcloud compute instances create my-mesh-vm \\ --tags = mesh-vm \\ --machine-type = n1-standard-2 Obtain the cluster's Pod IP address range. export CLUSTER_POD_CIDR = $( gcloud container clusters list --format = json | jq -r '.[0].clusterIpv4Cidr' ) Create a firewall rule to allow ingress on port 80 from the cluster pods to the VM. gcloud compute firewall-rules create \"cluster-pods-to-vm\" \\ --source-ranges = $CLUSTER_POD_CIDR \\ --target-tags = mesh-vm \\ --action = allow \\ --rules = tcp:80","title":"Create the Virtual Machine"},{"location":"vm_workloads/#apply-configuration-to-the-virtual-machine","text":"Now it's time to configure the Virtual machine. In this example, we run a simple Python HTTP server on port 80. You could configure any other service on a different port. Just make sure you configure the security and firewall rules accordingly. Copy the files from vm-files folder to the home folder on the instance. gcloud compute scp vm-files/* my-mesh-vm:~ cluster.env 100% 627 626.9KB/s 00:00 hosts 100% 36 50.7KB/s 00:00 istio-token 100% 905 1.4MB/s 00:00 mesh.yaml 100% 668 918.7KB/s 00:00 root-cert.pem 100% 1094 1.7MB/s 00:00 SSH into the instance and copy the root certificate to /etc/certs (you can use the command from the instance details page in the SSH dropdown): gcloud compute ssh my-mesh-vm sudo mkdir -p /etc/certs sudo cp root-cert.pem /etc/certs/root-cert.pem Copy the istio-token file to /var/run/secrets/tokens folder: sudo mkdir -p /var/run/secrets/tokens sudo cp istio-token /var/run/secrets/tokens/istio-token Download and install the Istio sidecar package: curl -LO https://storage.googleapis.com/istio-release/releases/1.18.2/deb/istio-sidecar.deb sudo dpkg -i istio-sidecar.deb Copy cluster.env to /var/lib/istio/envoy/ : sudo cp cluster.env /var/lib/istio/envoy/cluster.env Copy Mesh config ( mesh.yaml ) to /etc/istio/config/mesh : sudo cp mesh.yaml /etc/istio/config/mesh Add the istiod host to the /etc/hosts file: sudo sh -c 'cat $(eval echo ~$SUDO_USER)/hosts >> /etc/hosts' Change the ownership of files in /etc/certs and /var/lib/istio/envoy to the Istio proxy: sudo mkdir -p /etc/istio/proxy sudo chown -R istio-proxy /var/lib/istio /etc/certs /etc/istio/proxy /etc/istio/config /var/run/secrets /etc/certs/root-cert.pem With all files in place, we are ready to start Istio on the virtual machine. Because we used the VM auto-registration, Istio automatically creates the WorkloadEntry resource for us shortly after we start the Istio service on the VM. You can see the WorkloadEntry creation in action as follows: Watch for Workloadentry resources using the --watch flag kubectl get workloadentry -n vm-namespace --watch On the VM, start the Istio service: sudo systemctl start istio You should see the WorkloadEntry appear NAME AGE ADDRESS hello-vm-10.128.0.7 12m 10.128.0.7 Press Ctrl-C to stop watching for changes. On the VM, you can check that the istio service is running with systemctl status istio . Alternatively, we can look at the contents of the /var/log/istio/istio.log to see that the proxy was successfully started. At this point, the virtual machine is configured to talk with the Istio's control plane in the Kubernetes cluster.","title":"Apply configuration to the Virtual Machine"},{"location":"vm_workloads/#access-services-from-the-virtual-machine","text":"From a different terminal window, we can now deploy a Hello world application to the Kubernetes cluster Click for hello-world.yaml hello-world.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : apps/v1 kind : Deployment metadata : name : hello-world labels : app : hello-world spec : replicas : 1 selector : matchLabels : app : hello-world template : metadata : labels : app : hello-world spec : containers : - image : gcr.io/tetratelabs/hello-world:1.0.0 imagePullPolicy : Always name : svc ports : - containerPort : 3000 --- kind : Service apiVersion : v1 metadata : name : hello-world labels : app : hello-world spec : selector : app : hello-world ports : - port : 80 name : http targetPort : 3000 Save the above YAML to hello-world.yaml and deploy it: kubectl apply -f hello-world.yaml Wait for the Pods to become ready and then go back to the virtual machine and try to access the Kubernetes service: curl http://hello-world.default Hello World You can access any service running within your Kubernetes cluster from the virtual machine.","title":"Access services from the virtual machine"},{"location":"vm_workloads/#run-services-on-the-virtual-machine","text":"We can also run a workload on the virtual machine. Switch to the virtual machine instance and run a simple Python HTTP server: sudo python3 -m http.server 80 Serving HTTP on 0.0.0.0 port 80 (http://0.0.0.0:80/) ... What we want to do next is add the workload (Python HTTP service) to the mesh. For that reason, we created the VM namespace earlier. So let's create a Kubernetes service that represents the VM workload. Note that the name and the label values equal to the value of the VM_APP environment variable we set earlier. Don't forget to deploy the service to the VM_NAMESPACE . hello-vm-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Service metadata : name : hello-vm namespace : vm-namespace labels : app : hello-vm spec : ports : - port : 80 name : http-vm targetPort : 80 selector : app : hello-vm Save the above YAML to hello-vm-service.yaml and deploy it to the VM namespace: kubectl apply -f hello-vm-service.yaml We can now use the Kubernetes service name hello-vm.vm-namespace to access the workload running on the virtual machine (the Python server). Let's run a Pod inside the cluster and try to access the service from there: kubectl run curl --image = radial/busyboxplus:curl -i --tty --rm If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ After you get the command prompt in the Pod, you can run curl and access the workload. You should see a directory listing response. Similarly, you will notice a log entry on the instance where the HTTP server is running: [ root@curl:/ ] $ curl hello-vm.vm-namespace <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dt d\"> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"> <title>Directory listing for /</title> </head> <body> <h1>Directory listing for /</h1> <hr> ...","title":"Run services on the virtual machine"}]}